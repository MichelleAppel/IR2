{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic and advanced information retrieval concepts. You will implement different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a Indri index. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-three assignment**, the deadline is **Wednesday, January 31st**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](piazza.com/university_of_amsterdam/spring2018/52041inr6y/home).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "\n",
    "You will need the following software packages (tested with Python 3.5 inside [Anaconda](https://conda.io/docs/user-guide/install/index.html)):\n",
    "   * Python 3.5 and Jupyter\n",
    "   * Indri + Pyndri (Follow the installation instructions [here](https://github.com/nickvosk/pyndri/blob/master/README.md))\n",
    "   * gensim [link](https://radimrehurek.com/gensim/install.html)\n",
    "   * TREC Eval [link](https://github.com/usnistgov/trec_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/usr/local/lib/python3.5/dist-packages')\n",
    "\n",
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 164597 documents in this collection.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AP890425-0001', (1360, 192, 363, 0, 880, 0, 200, 0, 894, 412, 92160, 3, 192, 0, 363, 34, 1441, 0, 174134, 0, 200, 0, 894, 412, 2652, 0, 810, 107, 49, 4903, 420, 0, 1, 48, 35, 489, 0, 35, 687, 192, 243, 0, 249311, 1877, 0, 1651, 1174, 0, 2701, 117, 412, 0, 810, 391, 245233, 1225, 5838, 16, 0, 233156, 3496, 0, 393, 17, 0, 2435, 4819, 930, 0, 0, 200, 0, 894, 0, 22, 398, 145, 0, 3, 271, 115, 0, 1176, 2777, 292, 0, 725, 192, 0, 0, 50046, 0, 1901, 1130, 0, 192, 0, 408, 0, 243779, 0, 0, 553, 192, 0, 363, 0, 3747, 0, 0, 0, 0, 1176, 0, 1239, 0, 0, 1115, 17, 0, 0, 585, 192, 1963, 0, 0, 412, 54356, 0, 773, 0, 0, 0, 192, 0, 0, 1130, 0, 363, 0, 545, 192, 0, 1174, 1901, 1130, 0, 4, 398, 145, 39, 0, 577, 0, 355, 0, 491, 0, 6025, 0, 0, 193156, 88, 34, 437, 0, 0, 1852, 0, 828, 0, 1588, 0, 0, 0, 2615, 0, 0, 107, 49, 420, 0, 0, 190, 7, 714, 2701, 0, 237, 192, 157, 0, 412, 34, 437, 0, 0, 200, 6025, 26, 0, 0, 0, 0, 363, 0, 22, 398, 145, 0, 200, 638, 126222, 6018, 0, 880, 0, 0, 161, 0, 0, 319, 894, 2701, 0, 0, 0, 301, 1200, 0, 363, 251, 430, 0, 207, 0, 76143, 1773, 0, 243779, 0, 0, 72030, 0, 55, 4903, 420, 0, 2701, 1496, 420, 0, 25480, 0, 420, 0, 0, 200, 0, 392, 2949, 0, 1738, 0, 61, 0, 71, 79, 0, 200, 903, 0, 188, 53, 6, 0, 476, 2, 0, 2028, 97, 334, 0, 0, 200, 178, 0, 0, 107, 49, 0, 214, 0, 0, 0, 114, 3866, 1505, 195, 79893, 574, 0, 198, 2160, 0, 192, 0, 420, 0, 384, 0, 2701, 0, 114, 6025, 1549, 74627, 0, 238, 0, 0, 0, 3729, 0, 192, 0, 79893, 0, 0, 729, 3141, 129, 0, 192, 196764, 39, 0, 0, 714, 63, 0, 55, 420, 3356, 0, 0, 117, 412, 0, 0, 79758, 0, 1901, 1130, 4067, 2133, 0, 0, 875, 72, 0, 0, 336, 2789, 0, 0, 25, 920, 121, 104, 0, 3162, 0, 0, 420, 0, 2178, 0, 0, 386, 192545, 159306, 0, 0, 0, 1914, 0, 200, 0, 1794, 0, 2654, 0, 0, 25480, 420, 0, 2795, 0, 0, 229690, 0, 32559, 0, 0, 392, 253919, 0, 0, 0, 0, 379, 0, 0, 114, 0, 553, 10, 0, 1128, 0, 23610, 248, 151, 0, 418, 0, 651, 0, 36, 0, 0, 645, 0, 0, 513, 0, 0, 25480, 420, 34, 0, 0, 0, 15, 0, 3348, 0, 3496, 0, 35, 687, 0, 1, 48, 0, 0, 2803, 0, 0, 714, 1274, 0, 114, 62, 1006, 70268, 1200, 2357, 0, 497, 0, 497, 125, 0, 913, 4647, 3985, 0, 0, 3370, 245233, 0, 0, 687, 0, 4, 1288, 0, 0, 0, 0, 715, 0, 0, 687, 583, 0, 0, 1627, 0, 0, 11, 357, 1359, 0, 849, 0, 0, 1518, 462, 245233, 0, 0, 0, 0, 0, 0, 171, 70268, 0))\n"
     ]
    }
   ],
   "source": [
    "example_document = index.document(index.document_base())\n",
    "print(example_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'new'), (2, 'percent'), (3, 'two'), (4, '1'), (5, 'people'), (6, 'million'), (7, '000'), (8, 'government'), (9, 'president'), (10, 'years'), (11, 'state'), (12, '2'), (13, 'states'), (14, 'three'), (15, 'time')]\n"
     ]
    }
   ],
   "source": [
    "token2id, id2token, _ = index.get_dictionary()\n",
    "print(list(id2token.items())[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['52', 'students', 'arrested', 'takeover', 'university', 'massachusetts', 'building', 'fifty', 'two', 'students', 'arrested', 'tuesday', 'evening', 'occupying', 'university', 'massachusetts', 'building', 'overnight', 'protest', 'defense', 'department', 'funded', 'research', 'new', 'york', 'city', 'thousands', 'city', 'college', 'students', 'got', 'unscheduled', 'holiday', 'demonstrators', 'occupied', 'campus', 'administration', 'building', 'protest', 'possible', 'tuition', 'increases', 'prompting', 'officials', 'suspend', 'classes', '60', 'police', 'riot', 'gear', 'arrived', 'university', 'massachusetts', '5', 'p', 'm', 'two', 'hours', 'later', 'bus', 'drove', 'away', '29', 'students', 'camped', 'memorial', 'hall', 'students', 'charged', 'trespassing', '23', 'students', 'arrested', 'lying', 'bus', 'prevent', 'leaving', 'police', '300', 'students', 'stood', 'building', 'chanting', 'looking', 'students', 'hall', 'arrested', '35', 'students', 'occupied', 'memorial', 'hall', '1', 'p', 'm', 'monday', 'declined', 'offer', 'meet', 'administrators', 'provosts', 'office', 'tuesday', 'morning', 'presented', 'list', 'demands', 'halt', 'defense', 'department', 'research', '25', '000', 'student', 'campus', '40', 'students', 'left', 'building', 'tuesday', 'morning', 'university', 'administrators', 'told', 'arrested', '5', 'p', 'm', 'university', 'spokeswoman', 'jeanne', 'hopkins', 'takeover', 'second', 'western', 'massachusetts', 'campus', 'seven', 'protesters', 'arrested', 'april', '19', 'charges', 'disorderly', 'conduct', 'trespassing', 'demonstrating', 'military', 'funded', 'research', 'campus', 'particularly', 'research', 'anthrax', 'research', 'university', 'non', 'classified', 'researchers', 'make', 'work', 'public', 'university', 'rules', '11', '6', 'million', '22', 'percent', 'grant', 'money', 'received', 'university', 'came', 'defense', 'department', '1988', 'school', 'chancellor', 'joseph', 'd', 'duffey', 'issued', 'statement', 'telling', 'students', 'research', 'continue', 'campus', 'school', 'administrators', 'decide', 'differently', 'policy', 'negotiated', 'students', 'duffey', 'latest', 'occupation', 'began', 'students', 'rallying', 'monday', 'student', 'union', 'military', 'research', 'marched', 'administration', 'building', 'ducked', 'memorial', 'hall', 'en', 'route', 'followed', 'members', 'local', 'chapter', 'american', 'friends', 'service', 'committee', 'contended', 'research', 'dangerous', 'town', 'promotes', 'militarism', 'banned', 'university', 'argued', 'purpose', 'anthrax', 'research', 'peaceful', 'strain', 'bacteria', 'non', 'virulent', 'study', 'school', '23', 'years', 'incident', 'amherst', 'health', 'board', 'scheduled', 'hearing', 'wednesday', 'question', 'safety', 'anthrax', 'research', 'tuesday', 'time', '1969', 'classes', 'city', 'college', 'new', 'york', 'canceled', 'student', 'protests', 'school', 'spokesman', 'charles', 'deciccio', 'protesters', 'demanding', 'face', 'face', 'meeting', 'gov', 'mario', 'cuomo', 'feared', 'tuition', 'college', '1', '250', 'increased', 'college', 'staff', 'reduced', 'state', 'budget', 'cuts', 'governor', 'immediate', 'comment', 'tuition', 'set', 'deciccio']\n"
     ]
    }
   ],
   "source": [
    "print([id2token[word_id] for word_id in example_document[1] if word_id > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query by tokens: ['university', '', 'massachusetts']\n",
      "Query by ids with stopwords: [200, 0, 894]\n",
      "Query by ids without stopwords: [200, 894]\n"
     ]
    }
   ],
   "source": [
    "query_tokens = index.tokenize(\"University of Massachusetts\")\n",
    "print(\"Query by tokens:\", query_tokens)\n",
    "query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "print(\"Query by ids with stopwords:\", query_id_tokens)\n",
    "query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "print(\"Query by ids without stopwords:\", query_id_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document AP890425-0001 has 13 word matches with query: \"university  massachusetts\".\n",
      "Document AP890425-0001 and query \"university  massachusetts\" have a 2.5% overlap.\n"
     ]
    }
   ],
   "source": [
    "matching_words = sum([True for word_id in example_document[1] if word_id in query_id_tokens])\n",
    "print(\"Document %s has %d word matches with query: \\\"%s\\\".\" % (example_document[0], matching_words, ' '.join(query_tokens)))\n",
    "print(\"Document %s and query \\\"%s\\\" have a %.01f%% overlap.\" % (example_document[0], ' '.join(query_tokens),matching_words/float(len(example_document[1]))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is certainly not everything Pyndri can do, it should give you an idea of how to use it. Please take a look at the [examples](https://github.com/cvangysel/pyndri) as it will help you a lot with this assignment.\n",
    "\n",
    "**CAUTION**: Avoid printing out the whole index in this Notebook as it will generate a lot of output and is likely to corrupt the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('51', 'Airbus Subsidies'), ('52', 'South African Sanctions'), ('53', 'Leveraged Buyouts'), ('54', 'Satellite Launch Contracts'), ('55', 'Insider Trading'), ('56', 'Prime (Lending) Rate Moves, Predictions'), ('57', 'MCI'), ('58', 'Rail Strikes'), ('59', 'Weather Related Fatalities'), ('60', 'Merit-Pay vs. Seniority'), ('61', 'Israeli Role in Iran-Contra Affair'), ('62', \"Military Coups D'etat\"), ('63', 'Machine Translation'), ('64', 'Hostage-Taking'), ('65', 'Information Retrieval Systems'), ('66', 'Natural Language Processing'), ('67', 'Politically Motivated Civil Disturbances'), ('68', 'Health Hazards from Fine-Diameter Fibers'), ('69', 'Attempts to Revive the SALT II Treaty'), ('70', 'Surrogate Motherhood'), ('71', 'Border Incursions'), ('72', 'Demographic Shifts in the U.S.'), ('73', 'Demographic Shifts across National Boundaries'), ('74', 'Conflicting Policy'), ('75', 'Automation'), ('76', 'U.S. Constitution - Original Intent'), ('77', 'Poaching'), ('78', 'Greenpeace'), ('79', 'FRG Political Party Positions'), ('80', '1988 Presidential Candidates Platforms'), ('81', 'Financial crunch for televangelists in the wake of the PTL scandal'), ('82', 'Genetic Engineering'), ('83', 'Measures to Protect the Atmosphere'), ('84', 'Alternative/renewable Energy Plant & Equipment Installation'), ('85', 'Official Corruption'), ('86', 'Bank Failures'), ('87', 'Criminal Actions Against Officers of Failed Financial Institutions'), ('88', 'Crude Oil Price Trends'), ('89', '\"Downstream\" Investments by OPEC Member States'), ('90', 'Data on Proven Reserves of Oil & Natural Gas Producers'), ('91', 'U.S. Army Acquisition of Advanced Weapons Systems'), ('92', 'International Military Equipment Sales'), ('93', 'What Backing Does the National Rifle Association Have?'), ('94', 'Computer-aided Crime'), ('95', 'Computer-aided Crime Detection'), ('96', 'Computer-Aided Medical Diagnosis'), ('97', 'Fiber Optics Applications'), ('98', 'Fiber Optics Equipment Manufacturers'), ('99', 'Iran-Contra Affair'), ('100', 'Controlling the Transfer of High Technology'), ('101', 'Design of the \"Star Wars\" Anti-missile Defense System'), ('102', \"Laser Research Applicable to the U.S.'s Strategic Defense Initiative\"), ('103', 'Welfare Reform'), ('104', 'Catastrophic Health Insurance'), ('105', '\"Black Monday\"'), ('106', 'U.S. Control of Insider Trading'), ('107', 'Japanese Regulation of Insider Trading'), ('108', 'Japanese Protectionist Measures'), ('109', 'Find Innovative Companies'), ('110', 'Black Resistance Against the South African Government'), ('111', 'Nuclear Proliferation'), ('112', 'Funding Biotechnology'), ('113', 'New Space Satellite Applications'), ('114', 'Non-commercial Satellite Launches'), ('115', 'Impact of the 1986 Immigration Law'), ('116', 'Generic Drug Substitutions'), ('117', 'Capacity of the U.S. Cellular Telephone Network'), ('118', 'International Terrorists'), ('119', 'Actions Against International Terrorists'), ('120', 'Economic Impact of International Terrorism'), ('121', 'Death from Cancer'), ('122', 'RDT&E of New Cancer Fighting Drugs'), ('123', 'Research into & Control of Carcinogens'), ('124', 'Alternatives to Traditional Cancer Therapies'), ('125', 'Anti-smoking Actions by Government'), ('126', 'Medical Ethics and Modern Technology'), ('127', 'U.S.-U.S.S.R. Arms Control Agreements'), ('128', 'Privatization of State Assets'), ('129', 'Soviet Spying on the U.S.'), ('130', 'Jewish Emigration and U.S.-USSR Relations'), ('131', 'McDonnell Douglas Contracts for Military Aircraft'), ('132', '\"Stealth\" Aircraft'), ('133', 'Hubble Space Telescope'), ('134', 'The Human Genome Project'), ('135', 'Possible Contributions of Gene Mapping to Medicine'), ('136', 'Diversification by Pacific Telesis'), ('137', 'Expansion in the U.S. Theme Park Industry'), ('138', 'Iranian Support for Lebanese Hostage-takers'), ('139', \"Iran's Islamic Revolution - Domestic and Foreign Social Consequences\"), ('140', 'Political Impact of Islamic Fundamentalism'), ('141', \"Japan's Handling of its Trade Surplus with the U.S.\"), ('142', 'Impact of Government Regulated Grain Farming on International Relations'), ('143', 'Why Protect U.S. Farmers?'), ('144', 'Management Problems at the United Nations'), ('145', 'Influence of the \"Pro-Israel Lobby\"'), ('146', 'Negotiating an End to the Nicaraguan Civil War'), ('147', 'Productivity Trends in the U.S. Economy'), ('148', 'Conflict in the Horn of Africa'), ('149', 'Industrial Espionage'), ('150', 'U.S. Political Campaign Financing'), ('151', 'Coping with overcrowded prisons'), ('152', 'Accusations of Cheating by Contractors on U.S. Defense Projects'), ('153', 'Insurance Coverage which pays for Long Term Care'), ('154', 'Oil Spills'), ('155', 'Right Wing Christian Fundamentalism in U.S.'), ('156', 'Efforts to enact Gun Control Legislation'), ('157', 'Causes and treatments of multiple sclerosis (MS)'), ('158', 'Term limitations for members of the U.S. Congress'), ('159', 'Electric Car Development'), ('160', 'Vitamins - The Cure for or Cause of Human Ailments'), ('161', 'Acid Rain'), ('162', 'Automobile Recalls'), ('163', 'Vietnam Veterans and Agent Orange'), ('164', 'Generic Drugs - Illegal Activities by Manufacturers'), ('165', 'Tobacco company advertising and the young'), ('166', 'Standardized testing and cultural bias'), ('167', 'Regulation of the showing of violence and explicit sex in motion picture theaters, on television, and on video cassettes.'), ('168', 'Financing AMTRAK'), ('169', 'Cost of Garbage/Trash Removal'), ('170', 'The Consequences of Implantation of Silicone Gel Breast Devices'), ('171', \"Use of Mutual Funds in an Individual's Retirement Strategy\"), ('172', 'The Effectiveness of Medical Products and Related Programs Utilized in the Cessation of Smoking.'), ('173', 'Smoking Bans'), ('174', 'Hazardous Waste Cleanup'), ('175', 'NRA Prevention of Gun Control Legislation'), ('176', 'Real-life private investigators'), ('177', 'English as the Official Language in U.S.'), ('178', 'Dog Maulings'), ('179', 'U. S. Restaurants in Foreign Lands'), ('180', 'Ineffectiveness of U.S. Embargoes/Sanctions'), ('181', 'Abuse of the Elderly by Family Members, and Medical and Nonmedical Personnel, and Initiatives Being Taken to Minimize This Mistreatment'), ('182', 'Commercial Overfishing Creates Food Fish Deficit'), ('183', 'Asbestos Related Lawsuits'), ('184', 'Corporate Pension Plans/Funds'), ('185', 'Reform of the U.S. Welfare System'), ('186', 'Difference of Learning Levels Among Inner City and More Suburban School Students'), ('187', 'Signs of the Demise of Independent Publishing'), ('188', 'Beachfront Erosion'), ('189', 'Real Motives for Murder'), ('190', 'Instances of Fraud Involving the Use of a Computer'), ('191', 'Efforts to Improve U.S. Schooling'), ('192', 'Oil Spill Cleanup'), ('193', 'Toys R Dangerous'), ('194', 'The Amount of Money Earned by Writers'), ('195', 'Stock Market Perturbations Attributable to Computer Initiated Trading'), ('196', 'School Choice Voucher System and its effects upon the entire U.S. educational program'), ('197', 'Reform of the jurisprudence system to stop juries from granting unreasonable monetary awards'), ('198', 'Gene Therapy and Its Benefits to Humankind'), ('199', 'Legality of Medically Assisted Suicides'), ('200', 'Impact of foreign textile imports on U.S. textile industry')])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    print(parse_topics([f_topics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of 𝛌 in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of 𝛍 [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of 𝛅 in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of “soft” passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use 𝛔 equal to 50, and Dirichlet smoothing with 𝛍 optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand who the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Don’t forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering statistics about 456 terms.\n",
      "Inverted index creation took 36.36901903152466 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "# inverted index creation.\n",
    "start_time = time.time()\n",
    "\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "\n",
    "total_terms = 0\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "\n",
    "avg_doc_length = total_terms / num_documents\n",
    "\n",
    "print('Inverted index creation took', time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieval(model_name, score_fn, fpath_qrel = './ap_88_89/qrel_test', print_info = False):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "\n",
    "    if os.path.exists(run_out_path):\n",
    "        print(\"warning: the file {0} already exists and is not overwritten.\".format(run_out_path))\n",
    "        return\n",
    "\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    if print_info:\n",
    "        print('Retrieving using', model_name)\n",
    "    \n",
    "    data = build_data(fpath_qrel, score_fn)\n",
    "\n",
    "\n",
    "    # TODO: fill the data dictionary. \n",
    "    # The dictionary data should have the form: query_id --> (document_score, external_doc_id)\n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for reading data\n",
    "\n",
    "def parse_query_relevance_file(fpath):\n",
    "    with open(fpath, 'r') as rels:\n",
    "        doc_ids = []\n",
    "        query_id = None\n",
    "        for rel_line in rels:\n",
    "            q_d_rel = rel_line.split()\n",
    "            q_id = q_d_rel[0]\n",
    "            doc_id = q_d_rel[2]\n",
    "            if not query_id:\n",
    "                query_id = q_id\n",
    "                doc_ids.append(doc_id)\n",
    "            elif query_id == q_id:\n",
    "                doc_ids.append(doc_id)\n",
    "            else:\n",
    "                query_docs = (query_id, get_query_term_ids(query_id), doc_ids, get_int_document_ids(doc_ids))\n",
    "                query_id = q_id\n",
    "                doc_ids = []\n",
    "                yield query_docs\n",
    "\n",
    "def get_query_term_ids(query_id):\n",
    "    query_text = queries[query_id]\n",
    "    query_terms = index.tokenize(query_text)\n",
    "    query_term_ids = [token2id.get(query_term,0) for query_term in query_terms if token2id.get(query_term,0) > 0]\n",
    "    return query_term_ids\n",
    "\n",
    "def get_int_document_ids(document_ids):\n",
    "    int_document_ids = [doc_index for _, doc_index in index.document_ids(document_ids)]\n",
    "    return int_document_ids\n",
    "\n",
    "def build_data(fpath, score_fn, top = 1000):                \n",
    "    data = {}\n",
    "    for (q_id, q_term_ids, doc_ids, int_doc_ids) in parse_query_relevance_file(fpath):\n",
    "        top_doc_ids = doc_ids[:top] if top else doc_ids\n",
    "        top_int_doc_ids = get_int_document_ids(top_doc_ids)\n",
    "        zipped_doc_ids = zip(top_doc_ids, top_int_doc_ids)\n",
    "        data[q_id] = tuple((score_fn(int_doc_id, q_term_ids), doc_id) for (doc_id, int_doc_id) in zipped_doc_ids)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tf\\_idf(t;d) = \\log (1 + tf(t;d))[log(n) - log(df(t))] $\n",
    "with $tf(t;d)$ the number of times term $t$ occurs in document $d$,\n",
    "$n$ the total number of documents,\n",
    "$df(t)$ the number of documents in which term $t$ occurs.\n",
    "\n",
    "Score of query is sum of $tf\\_idf$ weights of terms in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def tfidf(int_document_id, query_term_ids, document_term_freq = None):\n",
    "    \"\"\"\n",
    "    TFIDF scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: add comment(s)\n",
    "    def _tfidf(int_document_id, query_term_id):\n",
    "        document_term_freq = inverted_index[query_term_id].get(int_document_id, 0)\n",
    "        num_documents = index.maximum_document() - index.document_base()\n",
    "        num_documents_with_term = len(inverted_index[query_term_id])\n",
    "        tf = math.log(1 + document_term_freq)\n",
    "        idf = math.log(num_documents) - math.log(num_documents_with_term)\n",
    "        score = tf * idf\n",
    "        return score\n",
    "\n",
    "    return sum([_tfidf(int_document_id, qt_id) for qt_id in query_term_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need this for BM25, calculating each time is too expensive\n",
    "# TODO: better solution than making it a global?\n",
    "# ALREADY DEFINED EARLIER\n",
    "#avg_doc_length = np.mean([len(index.document(i)) \n",
    "#                                   for i in range(index.document_base(), index.maximum_document())])\n",
    "\n",
    "def BM25(int_document_id, query_term_ids, k1 = 1.5, b = 0.75):\n",
    "    \"\"\"\n",
    "    BM25 scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param lambda (𝛌): smoothing parameter in the range [0.1, 0.5, 0.9]\n",
    "    \"\"\"\n",
    "    \n",
    "    document_length = len(index.document(int_document_id)[1])\n",
    "    doc_length_normalization_term = ((1 - b) + b * (document_length / avg_doc_length))\n",
    "    \n",
    "    def _BM25(int_document_id, query_term_id):\n",
    "        \n",
    "        # idf\n",
    "        num_documents_with_term = len(inverted_index[query_term_id])\n",
    "        idf = math.log(num_documents) - math.log(num_documents_with_term)\n",
    "        \n",
    "        # tf adjusted for document length and saturation\n",
    "        document_term_freq = inverted_index[query_term_id].get(int_document_id, 0)\n",
    "        adjusted_term_freq = ((k1 + 1) * document_term_freq) / (k1 * doc_length_normalization_term + document_term_freq)\n",
    "\n",
    "        # score\n",
    "        return adjusted_term_freq * idf\n",
    "\n",
    "    unique_query_term_ids = set(query_term_ids)\n",
    "    return sum([_BM25(int_document_id, qt_id) for qt_id in unique_query_term_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75983583"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need this for jelinek_mercer, dirichlet_prior, absolute_discounting\n",
    "# TODO: better solution than making it a global?\n",
    "corpus_size = sum([len(index.document(i)[1]) for i in range(index.document_base(), index.maximum_document())])\n",
    "corpus_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make corpus term frequency for language models with query terms only\n",
    "corpus_term_freqs = collections.defaultdict(lambda: 0, key = 0)\n",
    "\n",
    "for query_key in inverted_index.keys():\n",
    "    for document_key in inverted_index[query_key].keys():\n",
    "        corpus_term_freqs[query_key] += inverted_index[query_key][document_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make total corpus term frequency for language models\n",
    "total_corpus_term_freqs = collections.defaultdict(lambda: 0, key = 0)\n",
    "\n",
    "for document_index in range(index.document_base(), index.maximum_document()):\n",
    "    document = index.document(document_index)[1]\n",
    "    for term in document:\n",
    "        total_corpus_term_freqs[term] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jelinek_mercer(int_document_id, query_term_ids, lambda_ = 0.1):\n",
    "    \"\"\"\n",
    "    Jelinek-Mercer scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param lambda (𝛌): smoothing parameter in the range [0.1, 0.5, 0.9]\n",
    "    \"\"\"\n",
    "    \n",
    "    document_length = len(index.document(int_document_id)[1])\n",
    "    if document_length == 0:\n",
    "        return 0\n",
    "\n",
    "    # TODO: add comment(s)\n",
    "    def _jelinek_mercer(int_document_id, query_term_id):\n",
    "        document_term_freq = inverted_index[query_term_id].get(int_document_id, 0)\n",
    "        corpus_term_freq = corpus_term_freqs[query_term_id]\n",
    "        background_lm = corpus_term_freq / corpus_size\n",
    "        score = lambda_ * (document_term_freq / document_length) + (1 - lambda_) * background_lm\n",
    "        return score\n",
    "    \n",
    "    unique_query_term_ids = set(query_term_ids)\n",
    "    return np.sum([np.log(_jelinek_mercer(int_document_id, qt_id)) for qt_id in unique_query_term_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_prior(int_document_id, query_term_ids, mu = 500):\n",
    "    \"\"\"\n",
    "    Dirichlet Prior scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param mu (𝛍): smoothing parameter in the range [500, 1000, 1500]\n",
    "    \"\"\"\n",
    "    \n",
    "    document_length = len(index.document(int_document_id)[1])\n",
    "    if document_length == 0 and mu == 0:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    # TODO: add comment(s)a\n",
    "    def _dirichlet_prior(int_document_id, query_term_id):\n",
    "        document_term_freq = inverted_index[query_term_id].get(int_document_id, 0)\n",
    "        corpus_term_freq = corpus_term_freqs[query_term_id]\n",
    "        background_lm = corpus_term_freq / corpus_size\n",
    "        score = (document_term_freq + mu * background_lm) / (document_length + mu)\n",
    "        return score\n",
    "\n",
    "    # use log to avoid underflow\n",
    "    return np.prod([_dirichlet_prior(int_document_id, qt_id) for qt_id in set(query_term_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_discounting(int_document_id, query_term_ids, delta = 0.1):\n",
    "    \"\"\"\n",
    "    Absolute discounting scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param delta (𝛅): smoothing parameter in the range [0.1, 0.5, 0.9]\n",
    "    \"\"\"\n",
    "    \n",
    "    document_length = len(index.document(int_document_id)[1])\n",
    "    if document_length == 0:\n",
    "        return 0\n",
    "    \n",
    "    # TODO: add comment(s)\n",
    "    def _absolute_discounting(int_document_id, query_term_id):\n",
    "        document_term_freq = inverted_index[query_term_id].get(int_document_id, 0) \n",
    "        unique_words = len(set(index.document(int_document_id)))\n",
    "        corpus_term_freq = corpus_term_freqs[query_term_id]\n",
    "        background_lm = corpus_term_freq / corpus_size\n",
    "        score = (max(document_term_freq - delta, 0) / document_length\n",
    "                 + (delta * unique_words / document_length) * background_lm)\n",
    "        return score\n",
    "\n",
    "    # use log to avoid underflow\n",
    "    return np.sum([np.log(_absolute_discounting(int_document_id, qt_id)) for qt_id in set(query_term_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement positional language model\n",
    "\n",
    "# First we build an inverted index with positions,\n",
    "# only for the query terms and documents that are relevant for the \n",
    "# evaluation run\n",
    "def build_inverted_index_positions(fpath_qrel):\n",
    "\n",
    "    # collect relevant query_term_ids and int_doc_ids\n",
    "    query_term_ids = set()\n",
    "    int_doc_ids = set()\n",
    "    for query_data in parse_query_relevance_file(fpath_qrel):\n",
    "        query_term_ids = query_term_ids.union(query_data[1])\n",
    "        int_doc_ids = int_doc_ids.union(query_data[3])\n",
    "\n",
    "    # build inverted index with position info\n",
    "    inverted_index_positions = {}\n",
    "    for qt_id in query_term_ids:\n",
    "        for int_doc_id in int_doc_ids:\n",
    "            tf = inverted_index[qt_id].get(int_doc_id, 0)\n",
    "            if tf > 0:\n",
    "                doc_term_ids = np.array(index.document(int_doc_id)[1])\n",
    "                qt_positions = list(np.where(doc_term_ids == qt_id)[0])\n",
    "                if not qt_id in inverted_index_positions:\n",
    "                    inverted_index_positions[qt_id] = {}\n",
    "                inverted_index_positions[qt_id][int_doc_id] = qt_positions\n",
    "    return inverted_index_positions\n",
    "\n",
    "fpath_qrel_test = './ap_88_89/qrel_test'\n",
    "inverted_index_positions = build_inverted_index_positions(fpath_qrel_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel(i, j, sigma = 50):\n",
    "    numerator = -(i-j)**2.\n",
    "    denomineter = 2. * (sigma**2)\n",
    "    return np.exp(numerator/denomineter)\n",
    "\n",
    "def triangle_kernel(i, j, sigma = 50):\n",
    "    if abs(i - j) <= sigma:\n",
    "        return 1 - abs(i - j)/sigma\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def cosine_kernel(i, j, sigma = 50):\n",
    "    if abs(i - j) <= sigma:\n",
    "        return (1/2)*(1 + np.cos((abs(i-j)*np.pi)/(sigma)))\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def circle_kernel(i, j, sigma = 50):\n",
    "    if abs(i - j) <= sigma:\n",
    "        return (1 - (abs(i-j)/(sigma))**2)**(1/2)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def passage_kernel(i, j, sigma = 50):\n",
    "    if abs(i - j) <= sigma:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "def positional_language_model(\n",
    "    int_document_id, query_term_ids, inverted_index_positions, \n",
    "    fn_kernel, Zi_cache = {}, mu = 50, print_diagram = False):\n",
    "    \n",
    "    doc_length = len(index.document(int_document_id)[1])\n",
    "    if doc_length == 0:\n",
    "        return 0 # empty document\n",
    "\n",
    "    # For performance we assume that the kernel is defined in such a way \n",
    "    # that the best position lies within the range [min_pos, max_pos] \n",
    "    # if no query words occur in the document, we pick the position with the \n",
    "    # lowest Zi value, since that gives the highest background prob\n",
    "    positions = [ inverted_index_positions[qt_id].get(int_document_id, []) for qt_id in query_term_ids]\n",
    "    positions = reduce(lambda x, y: x+y, positions)\n",
    "    if not positions: # no query terms in document\n",
    "        positions = [0] # position with lowest Zi value is at the edge\n",
    "    pos_min = min(positions)\n",
    "    pos_max = max(positions) + 1\n",
    "    doc_positions = range(pos_min, pos_max)\n",
    "\n",
    "    \n",
    "    # Helper function to calculate the propagated probability of a \n",
    "    # single term at a given position (using Dirichlet smoothing)\n",
    "    fn_Zi = lambda pos_i: sum([fn_kernel(pos_i, pos_j) for pos_j in range(doc_length)])\n",
    "    def _term_prob_at_pos(query_term_id, pos_i):\n",
    "        qt_positions = inverted_index_positions[query_term_id].get(int_document_id, [])\n",
    "        qt_pos_i_propagated_count = sum([fn_kernel(pos_i, pos_j) for pos_j in qt_positions]) \n",
    "        \n",
    "        corpus_term_freq = corpus_term_freqs[query_term_id]\n",
    "        background_prob = corpus_term_freq / corpus_size\n",
    "        \n",
    "        # For performance we cache Zi values per document length and pos\n",
    "        if not Zi_cache.get(doc_length):\n",
    "            Zi_cache[doc_length] = {}\n",
    "        if not Zi_cache[doc_length].get(pos_i):\n",
    "            Zi_cache[doc_length][pos_i] = fn_Zi(pos_i)\n",
    "        zi_value_at_pos = Zi_cache[doc_length][pos_i]\n",
    "        \n",
    "        numerator = qt_pos_i_propagated_count + mu * background_prob\n",
    "        denominator = zi_value_at_pos + mu\n",
    "        qt_prob_pos_i = numerator/denominator\n",
    "        return qt_prob_pos_i\n",
    "\n",
    "    # We build a matrix with document positions as columns and\n",
    "    # query term indices as rows. The entries in this matrix represent\n",
    "    # the probability of the query term (row) at the given position (column) in the range pos_min, pos_max\n",
    "    qt_indices = range (len(query_term_ids))\n",
    "    pos_qt_matrix = np.array([ [\n",
    "        _term_prob_at_pos(query_term_ids[qt_index], pos) for pos in doc_positions\n",
    "    ] for qt_index in qt_indices])\n",
    "\n",
    "    if print_diagram:\n",
    "        for r in pos_qt_matrix:\n",
    "            plt.plot(doc_positions, r)\n",
    "        plt.show()\n",
    "        plt.plot(doc_positions, pos_qt_matrix.prod(axis = 0))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # We calculate the propability of the query at the relevant positions\n",
    "    # by multiplying the probability of the terms in the query\n",
    "    # (or actually we sum the log probs to avoid underflow) \n",
    "    query_probs_per_position = np.log(pos_qt_matrix).prod(axis = 0)\n",
    "        \n",
    "    # We return the query probability at the best position\n",
    "    query_prob_at_best_position = float(query_probs_per_position.max())\n",
    "    \n",
    "    return  query_prob_at_best_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3Xl8FeW9+PHPNycbCRAghC0sCRCRAIIYAwquqOBScAdaW7VYa4ta23vbq+2vvV5b2+u1bm21rXWpOyCKRqXiglqXsoR9h7CEBAIJW1iznXx/f8zEhpCQA5xkzvJ9v1555Zxnnpn5zkDO98w88zyPqCrGGGNMjNcBGGOMCQ2WEIwxxgCWEIwxxrgsIRhjjAEsIRhjjHFZQjDGGANYQjDGGOOyhGCMMQawhGCMMcYVG0glERkHPAH4gGdU9X8bLE8AXgTOAnYDE1V1i7vsPmAK4AfuVtU5bvmPgdsABVYAt6pqxfHi6Ny5s2ZkZAR6bMYYE/UWLVq0S1XTAqnbbEIQER/wJHApUAwsFJE8VV1dr9oUYK+q9heRScBDwEQRyQYmAYOAHsBHInIa0A24G8hW1SMiMsOt9/fjxZKRkUF+fn4gx2WMMQYQkcJA6wZyyygXKFDVTapaBUwDJjSoMwF4wX09ExgjIuKWT1PVSlXdDBS42wMnGbURkVggCdgeaNDGGGOCL5CEkA4U1Xtf7JY1WkdVa4ByILWpdVV1G/B7YCtQApSr6gcncwDGGGOCI5CEII2UNRwitak6jZaLSEecq4dMnFtJySJyU6M7F7ldRPJFJL+srCyAcI0xxpyMQBJCMdCr3vueHHt75+s67i2gFGDPcda9BNisqmWqWg28CZzb2M5V9WlVzVHVnLS0gNpFjDHGnIRAEsJCIEtEMkUkHqfxN69BnTzgZvf19cBcdSZayAMmiUiCiGQCWcACnFtFI0UkyW1rGAOsOfXDMcYYc7KafcpIVWtE5E5gDs5jp8+p6ioReQDIV9U84FngJREpwLkymOSuu8p9gmg1UANMVVU/MF9EZgKL3fIlwNPBPzxjjDGBknCaMS0nJ0ftsVNjjAmciCxS1ZxA6gbUMc0YY6LKrgIoXgjlxRATA+3ToefZkNrP68halCUEY4wB8NfA8unwrz9B6erG63QfCufcBYOvcxJFhLGEYIwxJcsh704oWQZdB8PlD0PfC6BjBmgt7NkMmz6FxS/Cm7c5SeOav0CXgV5HHlSWEIwx0W3pq/DOPdCmA1z/PAy6BqRBF6qu2c7PiDtg5Rvw/r3w1wtg/B9h6ERv4m4BkXfNY4wxgfrqT/DWD6D3CPjBVzD42mOTQX0xMXDGDfDDedArF2bdDp8/0nrxtjBLCMaY6PTF4/DBLyD7arjpTUjuHPi6bdOcdYbcCB8/4GwrAtgtI2NM9Fn6Knz0307j8DVPg+8kPgpj4512BPU724pPhtzvBT/WVmQJwRgTXYoWwDs/gszz4Zq/nlwyqBPjcxJK9RH4x39B5yzoe2GwIm11dsvIGBM9DuyAad9y+hXc8AL44k59m75YuPZpJxm8fgvs3XLq2/SIJQRjTHRQhbd+CJUHYPI0SOoUvG0ntINJrzqPqM64GfzVwdt2K7KEYIyJDgv+Bhs/hrG/gS6nB3/7qf2cx1BLlsJn/xf87bcCSwjGmMi3qwA+/CVkXQY5U1puP9kTYOhk+Pz3ULSw5fbTQiwhGGMimyq89xPwJTjf4I/XzyAYLn8I2vd0+jfUVLbsvoLMEoIxJrKtfAM2fwZjfgnturX8/hJT4KrHYPcG+PKJlt9fEFlCMMZEropyeP8+6HEm5Hy39fabdYkzBMY/fw+7N7befk+RJQRjTOT6/FE4VAZXPur0GWhNY38HsQnwj5+17n5PgSUEY0xkKi+G+X+BM26E9OGtv//23eGC/4KCj5yfMBBQQhCRcSKyTkQKROTeRpYniMh0d/l8Ecmot+w+t3ydiIx1ywaIyNJ6P/tF5J5gHZQxxvDJ75x+ARf9wrsYcr8HHfrAB7+CWr93cQSo2YQgIj7gSeByIBuYLCLZDapNAfaqan/gMeAhd91snPmVBwHjgKdExKeq61R1mKoOA84CDgOzgnRMxphot3M1LHsVcm+Hjn28iyM2AS65H0pXwbLXvIsjQIFcIeQCBaq6SVWrgGnAhAZ1JgAvuK9nAmNERNzyaapaqaqbgQJ3e/WNATaqauHJHoQxxhzlo/ud3sPn/YfXkTiNy+k5MPc3UHXI62iOK5CEkA4U1Xtf7JY1WkdVa4ByIDXAdScBoZ86jTHhoTgfNsyB0T8O7vAUJ0sExj4IB0pg3lNeR3NcgSSExnpxaIB1jruuiMQD44HXm9y5yO0iki8i+WVlZQGEa4yJav98GNp0hLNDaCjq3iPhtMvhqz86j8KGqEASQjHQq977nsD2puqISCyQAuwJYN3LgcWqurOpnavq06qao6o5aWlpAYRrjIlaJctg/fswcioktPU6mqNdeK+TDOb/1etImhRIQlgIZIlIpvuNfhKQ16BOHnCz+/p6YK6qqls+yX0KKRPIAhbUW28ydrvIGBMs//w9JKTAiNu9juRYPYbBgCvgX3+CI/u8jqZRzSYEt03gTmAOsAaYoaqrROQBERnvVnsWSBWRAuAnwL3uuquAGcBq4H1gqqr6AUQkCbgUeDO4h2SMiUqla2BNnpMMElO8jqZxIX6VIM4X+fCQk5Oj+fn5XodhjAlFb9wGa2fDj1eGRmNyU6Z9CzZ/DvcshzYdWnx3IrJIVXMCqWs9lY0x4W9fEax8E3JuDe1kAHDBz6CyHBY+43Ukx7CEYIwJf/P/4vwecYe3cQSi+1DoN8aJubrC62iOYgnBGBPeKvbD4hdh0NXQoVfz9UPB6HucQfeWvep1JEexhGCMCW9LXoLK/XDOVK8jCVzGedBjuNMvIYTGOLKEYIwJX/4amPcX6H0upJ/ldTSBE3GuEvZscp6MChGWEIwx4WtNHpRvDa+rgzqnXwWd+sEXjzvTfIYASwjGmPC14G/QMQMGXO51JCcuxgej7oaSpc4UnyHAEoIxJjztXAVbv4KcKa0/G1qwnDEJ2naFL//gdSSAJQRjTLha+AzEJsKZN3kdycmLS3QG4dv4MZSt8zoaSwjGmDBUsR+WTYfB14V+R7Tm5NwKvoR/96XwkCUEY0z4WT4dqg/B2VO8juTUJXd25n1e+hoc3uNpKJYQjDHhRdW5XdTjzPB61PR4Rv4Aao7A4hear9uCLCEYY8LLli+gbG1oTYBzqroOgswLnKem/NWehWEJwRgTXhY+A4kdYPC1XkcSXCN/CPu3edpRzRKCMSZ87C+Bte86TxbFtfE6muDKugw69YV5f/YsBEsIxpjwsfhFqK2JjMbkhmJiYMQPoHghFC30JgRP9mqMMSeq1u8MZNf3IuebdCQaNhkS2sO8pzzZfUAJQUTGicg6ESkQkXsbWZ4gItPd5fNFJKPesvvc8nUiMrZeeQcRmSkia0VkjYicE4wDMsZEqE2fQHkRnHVz83XDVUI7OPPbTjvC/pJW332zCUFEfMCTwOVANjBZRLIbVJsC7FXV/sBjwEPuutnAJGAQMA54yt0ewBPA+6p6OjAUZ75mY4xp3OIXISnVmag+kp09xbkaWvR8q+86kCuEXKBAVTepahUwDZjQoM4EoO4B2pnAGBERt3yaqlaq6magAMgVkfbA+cCzAKpapar7Tv1wjDER6WCZM1/y0MkQm+B1NC0rtR9kXQr5z0NNVavuOpCEkA4U1Xtf7JY1WkdVa4ByIPU46/YFyoDnRWSJiDwjIsmN7VxEbheRfBHJLysrCyBcY0zEWfYa1FY7t1OiQe734VApNStn8d7yEp78pKBVdhtIQpBGyhoO3t1UnabKY4HhwJ9V9UzgEHBM2wSAqj6tqjmqmpOWlhZAuMaYiKLq3C7qNQK6nO51NK1C+13EgeQ+rMl7hKmvLua1BVupqqlt8f0GkhCKgfoTlfYEtjdVR0RigRRgz3HWLQaKVXW+Wz4TJ0EYY8zRts6D3RtgeAQ3JtdTeqCCbz+Xz6P7zmdI7TqmX5XIZz+9iPjYln8oNJA9LASyRCRTROJxGokbdqXLA+r+ta4H5qqquuWT3KeQMoEsYIGq7gCKRGSAu84YYPUpHosxJhItfhHi28Ggq72OpMWtKC7nqj98QX7hHk4fdwcal8yIXW/gi2nsZkvwxTZXQVVrROROYA7gA55T1VUi8gCQr6p5OI3DL4lIAc6VwSR33VUiMgPnw74GmKqqdTNK3wW84iaZTcCtQT62r937xnJS28ZzZq+OjM7qTGJcmE6mYUy0qSiHVbNg6CSIb7SZMWIsKtzLLc8toH2bOGb9cBQDu7eH8kmw5GW49AFnVNQW1mxCAFDV2cDsBmW/qve6ArihiXUfBB5spHwpkHMiwZ6Man8tS4v2saH0IP5apV1iLLecm8Fto/uSkhTX0rs3xpyKFTOdUUCHf8frSFrUosI9fPvZBXRpl8Cr3xtJjw7usBy534P8Z52rpPN+0uJxiIbI5M6ByMnJ0fz8/JNat7LGz/xNe5i2cCuzV+ygc9sEfnftEC7N7hrkKI0xQfPXC5xn8u/4HKR1bpu0tq27D3P1U1+S0iaO6bePpEv7xKMrvPAN2L0JfrQMfAF9hz+KiCxS1YC+fEfN0BUJsT7OPy2Np751Fu/eNZq0dgl878V8HvlgHbW14ZMUjYkaJcucCeiHfydik0H5kWpu/fsC/LXKc7ecfWwyALjgv+CS+1slnhNPNxFgcHoKb08dxa/eXskf5xawbd8RHr5+aKs13BhjArD4JWdqyTMavRsd9lSVn76+jMLdh3lpyggyOzfRRpIxutViisqEABAfG8Pvrh1Cjw5tePTD9QA8csNQJEK/iRgTVqqPwPIZkD0B2nT0OpoW8eqCrXyweif/78qBnNMv1etwgChOCAAiwt1jslCFxz5aT6+OSfz40tO8DssYszoPKssjtjF5w84D/Prd1ZyX1Znvjsr0OpyvRXVCqHP3mP4U7T3MEx9vIKNzEtec2dPrkIyJbotfdIa4bsXbJa3FX6v858zlJMXH8sgNQ4kJoVvVUdOofDwiwm+vGcLIvp24940VrN95wOuQjIleuwqg8Atn3KIIvIX70r+2sKxoH//9jezGG5E9ZAnBFR8bwx8mn0m7xFjufm0JFdX+5lcyxgTfkpdAfDDsm15HEnQl5Ud4eM46zsvqzPihPbwO5xiWEOrp0i6Rh28YytodB/jdbJuewZhW56+Gpa/CaWOhXTevowm637y7Br8qD149JCQfYLGE0MBFA7pw66gMXvhXIQs27/E6HGOiy/o5cKg0IhuTFxXu4b0VJdxxQT96pyZ5HU6jLCE04qdjB5DeoQ0/n7WCyhq7dWRMq1nyErTtBv0v9TqSoFJVfvPeGrq0S+D280N3PmhLCI1Iio/lN9cMpqD0IH/5dJPX4RgTHfZvhw0fwJnfOqkhGkLZu8tLWLJ1H/952QCS4kP32CwhNOGiAV246ozuPPlpAUV7DnsdjjGRb+kroLVw5k1eRxJUlTV+Hnp/Lad3a8d1Z4X2I+2WEI7jF1cOJEbgoffXeh2KMZGtttYZqiLjPKf/QQR5Zd5Wivce4RdXDgz54XEsIRxH95Q23H5+P95dXsKiQmtgNqbFbPkc9hVG3KxoFdV+/vLZRkb27cR5WaE/BbAlhGbccUFfurRL4IF31xBOQ4UbE1YWvwiJHWDgN7yOJKheW7CV0gOV/GhMeAyJE1BCEJFxIrJORApE5N5GlieIyHR3+XwRyai37D63fJ2IjK1XvkVEVojIUhE5uUkOWkFSfCz/edkAlhXt46M1pV6HY0zkObwH1uTBGTdCXGj13D0VFdV+/vzpRkZkdgqZweua02xCEBEf8CRwOZANTBaR7AbVpgB7VbU/8BjwkLtuNs50moOAccBT7vbqXKSqwwKdvMEr1w5PJyM1iUc/XG9zJxgTbMtngL8q4voeTKu7Orgky+tQAhbIFUIuUKCqm1S1CpgGTGhQZwLwgvt6JjBGnG54E4BpqlqpqpuBAnd7YSXWF8OPLsliTcl+5qza4XU4xkQOVed2UY8zodsQr6MJmsoaP3/+bCO5mZ04p294XB1AYAkhHSiq977YLWu0jqrWAOVAajPrKvCBiCwSkdtPPPTWNX5oOn3TknnsI7tKMCZoti+G0lXOQHYR5O0l29m5v5K7Lu4fkkNUNCWQhNDY0TT8RGyqzvHWHaWqw3FuRU0VkfMb3bnI7SKSLyL5ZWVlAYTbMnwxwj2XnMb6nQd5b0WJZ3EYE1EWvwixbWDI9V5HEjSqyt8+38TA7u0Z3b+z1+GckEASQjHQq977nsD2puqISCyQAuw53rqqWve7FJhFE7eSVPVpVc1R1Zy0NG8f27pySHeyurTlj3M32FWCMaeq6hCseAMGXQOJKV5HEzSfri9jQ+lBvndeZlhdHUBgCWEhkCUimSISj9NInNegTh5Q9wDx9cBcdZ7RzAMmuU8hZQJZwAIRSRaRdgAikgxcBqw89cNpWb4Y4QcX9mP9zoN8ut6eODLmlKx6C6oORFxj8jOfb6Jr+wSuOiP0hrduTrMJwW0TuBOYA6wBZqjqKhF5QETGu9WeBVJFpAD4CXCvu+4qYAawGngfmKqqfqAr8IWILAMWAO+p6vvBPbSW8Y2hPeiRkmhjHBlzqhb9HVKzoPdIryMJmlXby/myYDe3jsokPjb8unkFNMqSqs4GZjco+1W91xXADU2s+yDwYIOyTcDQEw02FMT5YphyXl9+/e5qFhXu5aw+kTkBuDEtascKKF4AY38bUbOiPfP5ZpLjfUzO7e11KCcl/FJYCJh0di9S2sTx9D83eh2KMeEp/zmITYShk72OJGhKyo/wzrLt3Oh+PoQjSwgnITkhlu+c04cPVu9kY9lBr8MxJrxUHnA6ow26FpI6eR1N0LzwVSG1qnx3VKbXoZw0Swgn6eZzM4jzxfDsF5u9DsWY8LLidag6CDnf9TqSoKmo9jN94VYuze5Kr06hORtaICwhnKTObRO4elgPZi3eRvnhaq/DMSY8qMLC55xeyT1DesSaEzJ7RQl7D1fz7ZEZXodySiwhnIKbz83gSLWfGflFzVc2xkBxPuxc4VwdRFBj8kvzCumblsyo/uEzTEVjLCGcgkE9UsjN6MSL87bgt45qxjQv/zmIbwtDGn0oMSyt3FbOkq37uGlEn7DriNaQJYRTdPO5GRTtOcLctdZRzZjjOrwHVr0JZ0yEhHZeRxM0L88rJDEuJuSnxwyEJYRTdNmgrnRPSeSFr7Z4HYoxoW3Za1BTATm3eh1J0JQfqeatpdu4elh62D5qWp8lhFMU54vhppF9+KJgFxt2HvA6HGNCU20tLHwWeuZG1DDXbywqpqK6lptG9vE6lKCwhBAEk3N7Ex8bw9/tKsGYxhV8BHs2wojvex1J0KgqL88r5MzeHRicHhmD81lCCIJOyfGMH9qDWUu2caDCHkE15hjz/wLtukN2w7m1wtdXG3ezadchvh0hVwdgCSFobhrZh8NVft5e2nBkcGOiXNl62Pgx5EwBX/jfZ6/z8rxCOibFccWQ7l6HEjSWEIJkaM8UBnZvz6vzt+KM/G2MAWDBX8EXD2fd4nUkQbP7YCUfrdnJdcN7khjna36FMGEJIUhEhG+O6M3qkv0sKy73OhxjQsORfbD0NaffQVtvJ7gKpllLtlHtV248u1fzlcOIJYQgunpYD5Lifbw6v9DrUIwJDUtfgepDEdeYPH1hEcN6deC0rpHTnwIsIQRVu8Q4xg/twTvLSthvjcsm2tX6Yf5fofe50D0spz9p1JKifWwoPcjECLs6AEsIQffNEb05Uu3n7SXbvA7FGG+tfQ/2FUbU1QHAjIVFtInzcdUZkdOYXCeghCAi40RknYgUiMi9jSxPEJHp7vL5IpJRb9l9bvk6ERnbYD2fiCwRkXdP9UBCxZD0FAb1aM8r1rhsopkqfPk4dMyAgd/wOpqgOVRZwzvLtnPlGd1plxg5T0zVaTYhiIgPeBK4HMgGJotIdoNqU4C9qtofeAx4yF03G5gEDALGAU+526vzI5x5miNGXePy2h0HWFK0z+twjPFG4VewbRGcexfERM5TOO+tKOFQlT8ibxdBYFcIuUCBqm5S1SpgGtCwd8kE4AX39UxgjDjD/k0ApqlqpapuBgrc7SEiPYErgWdO/TBCy4Rh6STH+3h1/lavQzHGG18+AUmdYdi3vI4kqF7PL6JvWjI5ETqXeiAJIR2oP+B/sVvWaB1VrQHKgdRm1n0c+BlQe8JRh7i2CbGMH5bOu8u3U37EGpdNlNm5GjbMcdoO4tp4HU3QbCw7yMIte7kxp1fYD3PdlEASQmNH3vDmeFN1Gi0XkauAUlVd1OzORW4XkXwRyS8rK2s+2hAxObcXFdW15C21xmUTZb76A8Qlwdm3eR1JUM3IL8IXI1w7vOH34cgRSEIoBurfMOsJNByf4es6IhILpAB7jrPuKGC8iGzBuQV1sYi83NjOVfVpVc1R1Zy0tPDp2DIkPYXs7u15bUGRNS6b6FFe7MyZPPxmSOrkdTRBU+2v5Y1F27hoQBe6tEv0OpwWE0hCWAhkiUimiMTjNBLnNaiTB9zsvr4emKvOp2AeMMl9CikTyAIWqOp9qtpTVTPc7c1V1ZuCcDwhQ0SYnNuL1SX7Wbltv9fhGNM6/vWk84TROT/0OpKg+mRtKbsOVkZsY3KdZhOC2yZwJzAH54mgGaq6SkQeEJHxbrVngVQRKQB+AtzrrrsKmAGsBt4HpqqqP/iHEZrGD0snMS6G1xZa47KJAgd2OlNknnEjdOjtdTRBNSO/iLR2CVw0IHzuUpyM2EAqqepsYHaDsl/Ve10BNDpJqqo+CDx4nG1/CnwaSBzhJqWNMxJi3tLt/OKKgSQnBHS6jQlPX/0B/FVw/k+9jiSoSvdX8Mm6Mr53Xl9ifZHdlzeyjy4ETM7tzcHKGt5bUeJ1KMa0nIOlzoxoQ26E1H5eRxNUMxcX469VbswJ/zmTm2MJoYXl9OlIv7Rkpi2w20Ymgn31B/BXRtzVgaryen4xuRmd6JvW1utwWpwlhBYmIkw6uzeLt+5jvc25bCLRwTL36uAG6Nzf62iCauGWvWzedSjihrluiiWEVnDt8HTifMK0BUXNVzYm3Hz+CNRURNzVAcD0hUW0TYjliiHdvA6lVVhCaAWpbRO4LLsbby4ppqI6ah6yMtFgbyHkPwtn3gSds7yOJqgOVFQze0UJ3xjag6T46HggxBJCK5mU24t9h6v5YPVOr0MxJng++S1IDFx4n9eRBN07y0o4Uu2PisbkOpYQWsmofp3p2bGNNS6byLFjJSyf7oxZ1L6H19EE3fT8Ik7r2pZhvTp4HUqrsYTQSmJihIk5vfhq424Kdx/yOhxjTt3H/wOJ7WH0j72OJOjW7TjAsqJ9ET2QXWMsIbSiG3J6ESNOQ5UxYW3jJ7DhAxj9E2gTeUNBT19YRJxPuHZ49NwuAksIrapbSiIXn96F1xcVU+2PuFG/TbTwV8P79zqzoY24w+togq6yxs+sJcVcmt2VTsnxXofTqiwhtLKJZ/em7EAlc9eWeh2KMSdn4TNQthbG/g7iIm/kz49Wl7L3cDU35kRH34P6LCG0sosGpNG1fYLdNjLh6dAu+OR30O9iGHC519G0iOn5RfRISeS8rMgeyK4xlhBaWawvhhvO6sWn60opKT/idTjGnJiPH4DqQzDufyECG1uL9x7m8w1lXJ/TC19M5B1fcywheODGnF7UKsxYWOx1KMYErvArWPyC026QNsDraFrEzEXO3+QNZ0VXY3IdSwge6J2axOj+nZmRX4S/1mZTM2GgugLy7nbmObjo515H0yL8tc5AdqP7d6ZXpySvw/GEJQSPTMrtxbZ9R/iiYJfXoRjTvM9/D7s3wFWPQ3yy19G0iC8KdrFt35GInxXteCwheKTukTbruWxC3s5V8MVjMHQy9B/jdTQtZsbCIjomxXFpdlevQ/FMQAlBRMaJyDoRKRCRextZniAi093l80Uko96y+9zydSIy1i1LFJEFIrJMRFaJyP8E64DCRUKsj2vPTOfD1TvZdbDS63CMaVxNJcz6PiR2gMuanPgw7O0+WMkHq3dwzZk9SYj1eR2OZ5pNCCLiA54ELgeygckikt2g2hRgr6r2Bx4DHnLXzQYmAYOAccBT7vYqgYtVdSgwDBgnIiODc0jhY1JuL2pqlTcWWeOyCVFzfwM7VsCEP0FyqtfRtJhZS7ZR7deovl0EgV0h5AIFqrpJVauAacCEBnUmAC+4r2cCY8QZAGQCME1VK1V1M1AA5KrjoFs/zv2JutbV/l3acXZGR6YvLEI16g7fhLpNn8FXf4Sc70ZsnwNwZkWbvrCIYb06MKBbO6/D8VQgCSEdqN+Lqtgta7SOqtYA5UDq8dYVEZ+ILAVKgQ9VdX5jOxeR20UkX0Tyy8rKAgg3vEw8uzebdh1iweY9XodizL8d3gOz7oDU/hF9qwhg8dZ9bCg9yKQovzqAwBJCY70zGn6dbapOk+uqql9VhwE9gVwRGdzYzlX1aVXNUdWctLTI6zl45ZDutEuMZZr1XDahotYPb9wGh3fBdX+D+Mh+BHPGwiKS4n1cNTTyhvA+UYEkhGKgfursCWxvqo6IxAIpwJ5A1lXVfcCnOG0MUadNvI+rh6Uze0UJ5YervQ7HGGfSm40fwxUPQ48zvY6mRR2srOGd5du56ozutE2IjlnRjieQhLAQyBKRTBGJx2kkzmtQJw+42X19PTBXnZviecAk9ymkTCALWCAiaSLSAUBE2gCXAGtP/XDC08Sze1FZU8tbS7d5HYqJdmvedfocDP8OnHWL19G0uHeXbedwlZ+JZ/f2OpSQ0GxCcNsE7gTmAGuAGaq6SkQeEJHxbrVngVQRKQB+AtzrrrsKmAGsBt4HpqqqH+gOfCIiy3ESzoeq+m5wDy18DE5PYUh6Cq8t2GqNy8Y7JcuddoP0s+CK33sdTat4bWER/bu0ZXjv6JkV7XgCukZS1dnA7AZlv6r3ugK4oYl1HwQebFC2HIjsa9ETNCm3F7+YtZJlxeVRNWWfCRF7C+FwukhSAAAYU0lEQVSV650Z0Ca+DLEJXkfU4lZuK2dZ0T7++xvZUTUr2vFYT+UQMX5oD9rE+Zi+0Houm1Z2eA+8fB3UVMBNb0Tk/MiNeWV+IYlxMVE3K9rxWEIIEe0S47jqjO7kLd3Oocoar8Mx0aKiHF65AfZthcnToMtAryNqFfsrqnlryXbGD+1BSps4r8MJGZYQQsik3N4cqvLzzrKGD3EZ0wIqyuGla6FkKVz/HPQ51+uIWs2sxds4Uu3nppF9vA4lpFhCCCHDe3cgq0tb65NgWt6RffDi1VCyDG58EQZe5XVErUZVeWV+IUPSUzijp7XX1WcJIYSICJNye7O0aB9rd+z3OhwTqfZthefGOWMUTXwJTr/S64ha1cIte1m/8yA3jbRHTRuyhBBirj0znXhfDNMW2FWCaQHbl8Azl8D+bXDTzIgeo6gpL88rpF1iLN+wnsnHsIQQYjomxzN2cDfeXFxMRbXf63BMJFkxE56/AnzxMOUD6Huh1xG1ul0HK/nHyhKuG96TpHjrmdyQJYQQ9K0RvdlfUcPb1nPZBEN1Bbz7Y3hjCnQbArd9HDVPEzX0en4x1X7lWyPsdlFjLCGEoBGZnTi9Wzue/3KL9Vw2p6ZkuXOLKP85GPUjuOU9aBedM4L5a53G5BGZncjqGt3DXDfFEkIIEhFuHZXB2h0HmLfJhsU2J6G6Aj76H3j6Qji4E745Ay59AHzR+8z9h6t3Urz3CLecm+F1KCHLEkKImjAsnY5Jcfz9q81eh2LCiSqsmgVPjYAvHnXmQZ46H04b63Vknnv+y82kd2gT1XMmN8cSQohKjPMxObc3H67eSdGew16HY0KdqjPD2bOXweu3QFwSfPstuPpJSOrkdXSeW7mtnPmb93DzuX2I9dnHXlPszISwm0b2QUR4aV6h16GYUOWvgdV58MwYeHG808dg/B/hji+g30VeRxcynv9yC0nxPibmWGPy8dhzVyGsR4c2jBvcjWkLtnLPJVn2mJz5tz2bYMnLsPRVOFACHTPhqsedW0RxiV5HF1LKDlTyzrLtTMrtRUpS9LahBMI+YULcredm8N7yEt5YvI1v27gr0W3XBlj7HqybDUXzQWKg/6XOzGanXQ4++3NuzCvzC6ny13KzNSY3y/4Hhbiz+nRkaM8Unv18E9/M7Y0vxsZtjxrl26DwKyj8ErZ8DrsLnPLuQ+HiX8Kwb0bNUNUnq7LGz8vztnLRgDT6pbX1OpyQF1BCEJFxwBOAD3hGVf+3wfIE4EXgLGA3MFFVt7jL7gOmAH7gblWdIyK93PrdgFrgaVV9IihHFGFEhO9f0I8fvrKYD1bt4PIh3b0OyQRb5QFngppd62DHSmeMoR0r4OAOZ3lCe+g9EnK/7ww10aHX8bdnvvb20u3sOljJraMyvQ4lLDSbEETEBzwJXAoUAwtFJE9VV9erNgXYq6r9RWQS8BAwUUSyceZgHgT0AD4SkdOAGuA/VHWxiLQDFonIhw22aVxjB3WjT2oSf/lsI+MGd7PZnUKdKtRUQtUhqDoAh3fDoV1wqMz92eXc99+7xUkEh3f9e92YWEg73RlWovtQZ0jqbkMgxufRwYSv2lrlr59tZGD39pyX1dnrcMJCIFcIuUCBqm4CEJFpwASceZLrTADud1/PBP4kzqfWBGCaqlYCm905l3NV9V9ACYCqHhCRNUB6g20Gzx9zoLYafAnOOC6+OOd3bLz7vl6ZLwFiYpz7s+Jzf8c4f5ASAyJHlze6TIB6H9rHfIA3eN/Mcp8Ij/Tayz9WlFD43r/ISE0+hW3jfGD9+00Il9NEeSvEU1sD/mrn/42/+viv/ZVQeRCqDjrf9qsOOus3JS4Z2naBjhnOsNMdM5yfTv0gbUBUTF/ZGj5cs5ONZYf4w+Qz7UtUgAJJCOlA/aE3i4ERTdVR1RoRKQdS3fJ5DdZNr7+iiGTgzK88/wTiPjF9zoXqI+Cvcv+Aq5w/Yn+18y3OXwU1VW55FWgt1Pqd31oL6nc+JI4pd5e1ghwgJw7Ib5XdRaCmEnQT5TFxzpeEmNh/f2Fo6nV8MrTtCgntIL4tJLR1fte9btPJSQDJnSE5zalvWpSq8tSnG+nVqQ1XDO7mdThhI5CE0FhqbTjATlN1jruuiLQF3gDuUdVGJwAQkduB2wF69z7JZ4jH/+Hk1gtUY8ni3wuPrXt0QcDL//rZRp78pIDX7ziHAV3bBbBug2VNfUsK5AMyXMrtm6AB5m3aw7Kiffz66sHWEe0EBJIQioH6rVg9gYZzPNbVKRaRWCAF2HO8dUUkDicZvKKqbza1c1V9GngaICcnJzRHequ7VdTC93knnjeYJ74s5a/zd/PoRGtYNKYpf/lsI53bxnPDWT29DiWsBJI6FwJZIpIpIvE4jcR5DerkATe7r68H5qozTGceMElEEkQkE8gCFrjtC88Ca1T10WAcSDTokBTPpLN78/ay7WzdbcNZGNOYVdvL+Wx9GbeOyiQxzhrjT0SzCUFVa4A7gTnAGmCGqq4SkQdEZLxb7Vkg1W00/glwr7vuKmAGTmPx+8BUVfUDo4BvAxeLyFL354ogH1tE+v4FfYmNEf70yQavQzEmJD316UbaJsRyk3XkPGEB9UNQ1dnA7AZlv6r3ugK4oYl1HwQebFD2BY23L5hmdG2fyDdH9ObFfxUy9aL+9Em1Bkpj6qzbcYDZK0q444J+pLSxYSpOlLW2hKEfXNDPuUqYW+B1KMaElCc+Xk9yfCy3n9fX61DCkiWEMNTFvUp4c8k2Cncf8jocY0LCmpL9zF6xg1tHZdAxOd7rcMKSJYQwVXeV8Ee7SjAGgCc+2kC7hFhuG21XByfLEkKY6tI+kW+N6MOsJdvYssuuEkx0W7W9nPdX7eC7ozNtiOtTYAkhjN1xYV/ifTE88uF6r0MxxlP/9/462ifG8t3RNojdqbCEEMa6tEvktvMyeWfZdpYV7fM6HGM88cWGXXy2voy7Ls6yJ4tOkSWEMPf9C/qRmhzPb2evQY83dIUxEai2VvndP9aQ3qEN3z7H+h2cKksIYa5tQiz3XJLF/M17mLu21OtwjGlVecu2s2r7fn46doD1Sg4CSwgRYFJubzI7J/O7f6ylxl/b/ArGRICKaj8Pz1nHoB7tGT/UZo4LBksIESDOF8N/jTudgtKDvDyv0OtwjGkVf/vnJrbtO8LPrxhIjE0tGxSWECLE2EFdGd2/M498uJ6yA5Veh2NMiyree5gnPy3giiHdGNXfZkMLFksIEUJEuH/8ICqq/Tz0/lqvwzGmRf363dUIwi+uzPY6lIhiCSGC9O/Slu+OzmTmomIWFe71OhxjWsRn68uYs2ond17cn/QObbwOJ6JYQogwd1+cRbf2ifzyrZVUWwOziTAV1X7uz1tFZudkbjvPOqEFmyWECJOcEMv947NZXbKfp/+5yetwjAmqxz/awOZdh/j1hMEkxNpjpsFmCSECjRvcnSuHdOeJjzawYecBr8MxJiiWFe3j6X9uZNLZvRidZQ3JLcESQoS6f/wgkhN8/HTmcvy11oPZhLeqmlp+NnM5Xdol8vMrB3odTsQKKCGIyDgRWSciBSJybyPLE0Rkurt8vohk1Ft2n1u+TkTG1it/TkRKRWRlMA7EHC2tXQL3jx/E0qJ9PPuF3Toy4e1PczewbucBHrxmMO0TbbyiltJsQhARH/AkcDmQDUwWkYbPek0B9qpqf+Ax4CF33WxgEjAIGAc85W4P4O9umWkh44f2YOygrjw8Zx0rt5V7HY4xJ2X+pt386ZMCrhvekzEDu3odTkQL5AohFyhQ1U2qWgVMAyY0qDMBeMF9PRMYIyLilk9T1UpV3QwUuNtDVf8J7AnCMZgmiAgPXXcGndsmcNdrSzhYWeN1SMackH2Hq7hn+lJ6d0rifyYM8jqciBdIQkgHiuq9L3bLGq2jqjVAOZAa4LqmBXVIiufxicMo3H2I/357ldfhGBMwVeVnM5ez62Alf5w8nLYJsV6HFPECSQiNDRLSsJWyqTqBrHv8nYvcLiL5IpJfVlZ2Iqsa14i+qdx1cRZvLC5mRn5R8ysYEwKe/3ILH6zeyX+NO50hPVO8DicqBJIQioFe9d73BLY3VUdEYoEUnNtBgax7XKr6tKrmqGpOWlraiaxq6rnr4v6M6p/K/5u1ksVbrRezCW1fFuziwdlruCy7K98dZR3QWksgCWEhkCUimSISj9NInNegTh5ws/v6emCuOrO15AGT3KeQMoEsYEFwQjcnItYXw58mD6dbSiLff2kRO8orvA7JmEYV7j7ED19ZTL+0ZB6dOMxGMm1FzSYEt03gTmAOsAaYoaqrROQBERnvVnsWSBWRAuAnwL3uuquAGcBq4H1gqqr6AUTkNeBfwAARKRaRKcE9NNNQx+R4nrk5h8OVNXz/pXyOVPm9DsmYo+yvqOZ7L+YD8Lfv5Fi7QSuTcJp2MScnR/Pz870OI+x9uHon338pnwtOS+Pp7+QQ57P+icZ7FdV+bn5uAYsK9/L3W3OtN3KQiMgiVc0JpK59EkShS7O78uA1Q/hkXRk/m7mcWuvJbDxW46/lrteWMH/zHh65caglA4/Y9ViUmpzbmz2Hqnh4zjpS2sTx39/Ixuk6Ykzr8tcq9765gg9X7+T+b2QzYZg9me4VSwhR7IcX9mPf4Sr+9vlmqv21/HrCYGvAM62qxl/Lf76+jLeWbudHY7K4xZ4o8pQlhCgmIvz8ioHE+WJ46tONHKny83/Xn0GstSmYVlBVU8vdry3h/VU7+OnYAUy9qL/XIUU9SwhRTkT42bjTaRPn45EP17O/oprHJ51pT3eYFlV+uJqpry7mi4Jd/PKqbKaMtiuDUGBfBQ0Ad43J4oEJg/hkXRnX//krivce9jokE6E27zrENU99yfzNu3n4+jMsGYQQSwjma985J4PnbzmbbfuOcPWTX/LVxl1eh2QizCfrSrn6yS/Ze7iKV24byQ05vZpfybQaSwjmKOeflsasH46ifZs4vvXMfB6es9bmZjanrLLGzwPvrObW5xfSPSWRt6eOJjezk9dhmQYsIZhj9O/SlnfvGs2NZ/XiyU82csNf/sV6m4rTnKSV28q55smveO7LzdxybgZvTR1F79Qkr8MyjbCeyua43l2+nV++tZIDFTXcfn5f7h6TRWKcTW5umneosoZHP1zP819uplNyAr+7dgiXZtsEN63tRHoq26Mk5riuOqMH5/brzG9nr+GpTzfy9tLt3HNJFtcO74nP+iyYRlT7a5mRX8QTH22g7GAl38ztzc/GnU5KG5v6MtTZFYIJ2LxNu/nt7DUsLy4nq0tb7h6TxeWDu1m/BQM4/QreWbadP87dwJbdh8np05GfXzmQ4b07eh1aVDuRKwRLCOaEqCrvr9zB7z9Yx8ayQ6R3aMOtozK4bnhPOibHex2e8cDug5XMyC/m719tZuf+Sk7v1o6fjh3Axad3seFQQoAlBNPiamuVj9eW8rfPN7Fg8x7ifMKY07ty3Vk9uXBAmo2gGuEqa/x8sraUmYu28em6UmpqldH9O3PbeZlccFqaJYIQYm0IpsXFxAiXZnfl0uyurN6+n5mLinl76TbeX7WDdomxnH9aGmNO78IFp6WR2jbB63BNEJQeqODTdWXMXVPKFwW7OFhZQ5d2CUwZncl1Z/XktK7tvA7RnCK7QjBBU+2v5Z/ry/hg1U7mriul7EAlAFld2pKT0ZGz+nTijJ4pZHZOtiuIEFdVU8umXQdZXlROfuEe8gv3sqnsEADd2idy8cAuXJbdldH9O1sbUoizW0YNfFz4MX4NbHawQC91heDWcyt7t+9mKM3/P6lfR2th695DrNq2n41lB9lYdsiZoU2U2JgYurZPIL1DG7qlJJLaNp7UtgmkJsfToU08gfwTBBRPAP+3T/S4jlMpOPsKQsyB7kcVyo9Us+dQFXsOVbH7UBWl+ysoKa+g9EAFfneejKQEH5mpyWR0Tub0bu1I79Dm67+TVv13CNJnVaj93wlkX4mxiYzvN77Zeo0JekIQkXHAE4APeEZV/7fB8gTgReAsYDcwUVW3uMvuA6YAfuBuVZ0TyDYbc7IJIfeVXI7UHDnh9YwxJhSkJqby6cRPT2rdoLYhiIgPeBK4FCgGFopInqqurldtCrBXVfuLyCTgIWCiiGQDk4BBQA/gIxE5zV2nuW0GzatXvBq8b4IE75tKS+470O3V1Q3kaiKgOgF8va+/ncoaP6UHKtm5v5Kd+49Qur+SPYer2H+khn1Hqtl/pIryw9UcqW5s+IxA9gWxvhhiY4S6bhMxMUKMG6eIEAOIuK/d8hhx4hTBKavblf77V8N/C9V/f2tWPap6vTrHvq77t6p7X6uCv7aW2lqlurYWvx9qTvT/nB59btom+OiYHE/7xFg6JMWT0iaOjknxdEtpQ7f2iXRPSaBL+0QSYo/udBjQVWZAVYL/f6el9xWI1jqu1mqkD6RRORcoUNVNACIyDZgA1P/wngDc776eCfxJnCOYAExT1Upgs4gUuNsjgG0GTf+ONs56SOvSfJWqmlqOVPk5WFXD4coaDlX5OVRZw6HKGo5U+6nxK9X+Wqr9tVTVva45+n2te5tEValVnPe472udD+a6cuotryur+5MUkXqv3d/1ln39/utl0kTdRpa5K8b5hNiYGGJ9QmyM++OLwRfjLPPFxPy7TozQJt5HcoKPNnGxJNW9jo8lKc5H28RYa7MxAQkkIaQDRfXeFwMjmqqjqjUiUg6kuuXzGqxbNz9ec9sEQERuB24H6N27dwDhmkgUHxtDfGwMKUnW29WYlhLI14bGrlUaXr82VedEy48tVH1aVXNUNSctLe24gRpjjDl5gSSEYqD+oOU9ge1N1RGRWCAF2HOcdQPZpjHGmFYUSEJYCGSJSKaIxOM0Euc1qJMH3Oy+vh6Yq06LWx4wSUQSRCQTyAIWBLhNY4wxrajZNgS3TeBOYA7OI6LPqeoqEXkAyFfVPOBZ4CW30XgPzgc8br0ZOI3FNcBUVadDQGPbDP7hGWOMCVRUdEwzxphodSL9EOxZNGOMMYAlBGOMMS5LCMYYY4Awa0MQkTKgsInFnYFdrRhOqLPzcSw7J0ez83GsSDwnfVQ1oE5cYZUQjkdE8gNtOIkGdj6OZefkaHY+jhXt58RuGRljjAEsIRhjjHFFUkJ42usAQoydj2PZOTmanY9jRfU5iZg2BGOMMacmkq4QjDHGnIKwSAgi8pyIlIrIynplnUTkQxHZ4P7u6JaLiPxBRApEZLmIDPcu8pYhIr1E5BMRWSMiq0TkR255NJ+TRBFZICLL3HPyP255pojMd8/JdHcwRdwBF6e752S+iGR4GX9LERGfiCwRkXfd99F+PraIyAoRWSoi+W5Z1P7dNBQWCQH4OzCuQdm9wMeqmgV87L4HuBxnVNUsnIl1/txKMbamGuA/VHUgMBKY6k5XGs3npBK4WFWHAsOAcSIyEmc618fcc7IXZ7pXqDftK/CYWy8S/QhYU+99tJ8PgItUdVi9x0uj+e/maKoaFj9ABrCy3vt1QHf3dXdgnfv6r8DkxupF6g/wNs781HZOnONLAhbjzMK3C4h1y88B5riv5wDnuK9j3XridexBPg89cT7gLgbexZmYKmrPh3tsW4DODcrs78b9CZcrhMZ0VdUSAPd33cy8jU35mU6Eci/tzwTmE+XnxL09shQoBT4ENgL7VLXGrVL/uI+a9hWom/Y1kjwO/Ayodd+nEt3nA5yZGT8QkUXu9LwQ5X839QUyp3K4CXh6znAnIm2BN4B7VHV/3QTvjVVtpCzizok6c20ME5EOwCxgYGPV3N8RfU5E5CqgVFUXiciFdcWNVI2K81HPKFXdLiJdgA9FZO1x6kbLOflaOF8h7BSR7gDu71K3PCqm5xSROJxk8IqqvukWR/U5qaOq+4BPcdpXOrjTusLRx93UtK+RYhQwXkS2ANNwbhs9TvSeDwBUdbv7uxTnS0Mu9nfztXBOCPWn7bwZ5z56Xfl33CcERgLldZeDkUKcS4FngTWq+mi9RdF8TtLcKwNEpA1wCU5j6ic407rCseeksWlfI4Kq3qeqPVU1A2cGw7mq+i2i9HwAiEiyiLSrew1cBqwkiv9ujuF1I0YgP8BrQAlQjZO1p+Dc3/wY2OD+7uTWFeBJnPvHK4Acr+NvgfMxGufSdTmw1P25IsrPyRnAEvecrAR+5Zb3xZnHuwB4HUhwyxPd9wXu8r5eH0MLnpsLgXej/Xy4x77M/VkF/MItj9q/m4Y/1lPZGGMMEN63jIwxxgSRJQRjjDGAJQRjjDEuSwjGGGMASwjGGGNclhCMMcYAlhCMMca4LCEYY4wB4P8DvrUWl3+DDgwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56e1b3b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56bee3d518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-235.61866712426212"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example PLM \n",
    "doc_id = 6422\n",
    "existing_qt_ids = [qt_id for qt_id in index.document(doc_id)[1] if inverted_index_positions.get(qt_id)]\n",
    "qt_1 = existing_qt_ids[5] # [112, 556]\n",
    "qt_2 = existing_qt_ids[25] # [507]\n",
    "qt_3 = 4204 # does not exist\n",
    "positional_language_model(\n",
    "    doc_id, [qt_1, qt_2, qt_3], inverted_index_positions, \n",
    "    gaussian_kernel, mu = 50, print_diagram = True)\n",
    "# 3.747485365662843e-05 for [qt_1, qt_2]\n",
    "# 4.281483411901026e-09 for [qt_1, qt_2, qt_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def generate_evaluation_results(retrieval_models, fpath_qrel, output_dir, print_info = False):\n",
    "    \n",
    "    # Generates <modelname>.run files (i.e. tfidf.run) \n",
    "    # containing the ranking result for the query and\n",
    "    # documents that occur in the fpath_qrel file\n",
    "    for mname, mfunc in retrieval_models.items():   \n",
    "        start_time = time.time()\n",
    "        run_retrieval(mname, mfunc, fpath_qrel) \n",
    "        if print_info:\n",
    "            print (\"retrieval for '{0}' took {1:.2f} secondes: \".format(mname, time.time() - start_time))\n",
    "    print()\n",
    "        \n",
    "    # Applies trec eval on <modelname>.run and fpath_qrel files\n",
    "    # to generate results/<modelname.txt> files.\n",
    "    # The result files contain scores per query using different \n",
    "    # metrics (i.e. NDCG@10, recall@100, ...)\n",
    "    for mname, mfunc in retrieval_models.items():     \n",
    "        if print_info:\n",
    "            print(\"Generating results for {0}\".format(mname))\n",
    "        os.system(\"mkdir -p {0}\".format(output_dir))\n",
    "        os.system(\"./trec_eval/trec_eval -m all_trec -q {0} {1}.run > {2}/{1}.txt\".format(\n",
    "            fpath_qrel, mname, output_dir))\n",
    "    print()\n",
    "        \n",
    "            \n",
    "def clean_evaluation_results(retrieval_models, output_dirs, print_info = False):\n",
    "\n",
    "    def _delete_file_if_exists(fpath):\n",
    "        try:\n",
    "            os.remove(fpath)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    for mname in retrieval_models.keys():\n",
    "        _delete_file_if_exists(\"{0}.run\".format(mname))\n",
    "        for d in output_dirs:\n",
    "            fpath = \"{0}/{1}.txt\".format(d, mname)\n",
    "            if print_info:\n",
    "                print(\"Deleting {0} if exists\".format(fpath))\n",
    "            _delete_file_if_exists(fpath)\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import itertools\n",
    "\n",
    "def build_df_results(retrieval_models, metrics, dirpath, print_info = False):\n",
    "    # Build dataframe with trec results\n",
    "    df_results = pd.DataFrame({})\n",
    "    for mname, mfunc in retrieval_models.items():\n",
    "        if print_info:\n",
    "            print(\"Reading results for {0}\".format(mname))\n",
    "        df_model = pd.read_csv(\n",
    "            '{0}/{1}.txt'.format(dirpath, mname), \n",
    "            delim_whitespace = True, \n",
    "            header = None, \n",
    "            names = ['metric', 'query_id', mname]\n",
    "        )\n",
    "        if len(df_results): # add column containing results for current model\n",
    "            df_results = pd.merge(\n",
    "                df_results, \n",
    "                df_model, \n",
    "                on=['metric', 'query_id'] \n",
    "#                 validate=\"one_to_one\" <- Raises error\n",
    "            )\n",
    "        else: # create data frame with scores for first model: 'metric', 'query_id', 'model name'\n",
    "            df_results = df_model\n",
    "\n",
    "    # filter rows on metrics\n",
    "    print(\"Filtering rows on: {0}\".format(\", \".join(metrics)))\n",
    "    df_results = df_results[df_results.apply(\n",
    "        lambda row: row['metric'] in metrics and not row['query_id'] == 'all', axis = 1)]\n",
    "    for mname in retrieval_models.keys():\n",
    "        df_results[mname] = df_results[mname].astype(np.float64) \n",
    "\n",
    "    print()\n",
    "    return df_results\n",
    "\n",
    "def analyze_evaluation_results(df_results, print_df = True, result_dir = None):\n",
    "\n",
    "    retrieval_models = list(df_results.columns[2:])\n",
    "    metrics = list(df_results['metric'].unique())\n",
    "\n",
    "    def _calculate_pvalues(df_results):\n",
    "\n",
    "        rm_pairs = []\n",
    "        pvalues = []\n",
    "        metric_names = []\n",
    "        for m in metrics:\n",
    "            for rm1, rm2 in itertools.product(retrieval_models, retrieval_models):\n",
    "                if rm1 > rm2:\n",
    "                    ttest_result = stats.ttest_rel(\n",
    "                        df_results[df_results.metric == m][rm1], \n",
    "                        df_results[df_results.metric == m][rm2]\n",
    "                    )\n",
    "                    metric_names.append(m)\n",
    "                    rm_pairs.append(\"{0}_{1}\".format(rm1, rm2))\n",
    "                    pvalues.append(ttest_result.pvalue)\n",
    "\n",
    "        d = {'metric': metric_names, 'retrieval_pairs': rm_pairs, 'pvalues': pvalues}\n",
    "        df_pvalues = pd.DataFrame( data = d)\n",
    "        return df_pvalues\n",
    "\n",
    "    df_means = df_results.groupby(['metric'])[retrieval_models].mean()\n",
    "    df_means.reset_index(inplace=True)\n",
    "    df_pvalues = _calculate_pvalues(df_results)\n",
    "    df_pvalues.sort_values(['retrieval_pairs', 'metric']).reset_index(inplace=True)\n",
    "    \n",
    "    if result_dir:\n",
    "        df_results.to_csv(\"{0}/results.csv\".format(result_dir), index = False)\n",
    "        df_means.to_csv(\"{0}/means.csv\".format(result_dir), index = False)\n",
    "        df_pvalues.to_csv(\"{0}/pvalues.csv\".format(result_dir), index = False)\n",
    "    \n",
    "    if print_df:\n",
    "        print()\n",
    "        print('means: ')\n",
    "        display(df_means)\n",
    "        print('pvalues: ')\n",
    "        display(df_pvalues)\n",
    "        print('ndcg: ')\n",
    "        display(df_results[df_results['metric'] == 'ndcg_cut_10'].head(20))\n",
    "    \n",
    "    return df_results, df_means, df_pvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maartje/anaconda3/envs/information_retrieval_1/lib/python3.5/site-packages/ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2289fbacf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "def plot_ndcg(lm_name, lm_fn, lambda_values):\n",
    "    \n",
    "    # NDCG@10 versus Hyperparameters\n",
    "    retrieval_models = {}\n",
    "    for l in lambda_values:\n",
    "        rm_name = \"{0}_ndcg_{1}\".format(lm_name, l)\n",
    "        rm_fn = lambda x,y,l=l : lm_fn(x, y, lambda_ = l)\n",
    "        retrieval_models[rm_name] = rm_fn\n",
    "\n",
    "    clean_evaluation_results(retrieval_models, output_dirs=[\"ndcg\"])\n",
    "    generate_evaluation_results(retrieval_models, './ap_88_89/qrel_validation', \"ndcg\")\n",
    "\n",
    "    ndcg10_values = []\n",
    "    for mname in retrieval_models.keys():\n",
    "        grep_result = subprocess.run(['grep', '-E', '^ndcg_cut_10(\\s)*all', 'ndcg/{0}.txt'.format(mname)], stdout=subprocess.PIPE)\n",
    "        ndcg10_all = float(grep_result.stdout.split()[2])\n",
    "        ndcg10_values.append(ndcg10_all)\n",
    "\n",
    "    plt.plot(lambda_values, ndcg10_values)\n",
    "    plt.show()\n",
    "\n",
    "plot_ndcg('jm', jelinek_mercer, [ l/10. for l in range(0, 11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting results_test/jm_0.5.txt if exists\n",
      "Deleting results_test/dir_1000.txt if exists\n",
      "Deleting results_test/jm_0.1.txt if exists\n",
      "Deleting results_test/jm_0.9.txt if exists\n",
      "Deleting results_test/abs_0.9.txt if exists\n",
      "Deleting results_test/dir_1500.txt if exists\n",
      "Deleting results_test/dir_500.txt if exists\n",
      "Deleting results_test/abs_0.1.txt if exists\n",
      "Deleting results_test/abs_0.5.txt if exists\n",
      "\n",
      "Retrieving using jm_0.5\n",
      "retrieval for 'jm_0.5' took 2.62 secondes: \n",
      "Retrieving using dir_1000\n",
      "retrieval for 'dir_1000' took 2.35 secondes: \n",
      "Retrieving using jm_0.1\n",
      "retrieval for 'jm_0.1' took 2.59 secondes: \n",
      "Retrieving using jm_0.9\n",
      "retrieval for 'jm_0.9' took 2.59 secondes: \n",
      "Retrieving using abs_0.9\n",
      "retrieval for 'abs_0.9' took 6.44 secondes: \n",
      "Retrieving using dir_1500\n",
      "retrieval for 'dir_1500' took 2.35 secondes: \n",
      "Retrieving using dir_500\n",
      "retrieval for 'dir_500' took 2.35 secondes: \n",
      "Retrieving using abs_0.1\n",
      "retrieval for 'abs_0.1' took 6.74 secondes: \n",
      "Retrieving using abs_0.5\n",
      "retrieval for 'abs_0.5' took 6.49 secondes: \n",
      "\n",
      "Generating results for jm_0.5\n",
      "Generating results for dir_1000\n",
      "Generating results for jm_0.1\n",
      "Generating results for jm_0.9\n",
      "Generating results for abs_0.9\n",
      "Generating results for dir_1500\n",
      "Generating results for dir_500\n",
      "Generating results for abs_0.1\n",
      "Generating results for abs_0.5\n",
      "\n",
      "Reading results for jm_0.5\n",
      "Reading results for dir_1000\n",
      "Reading results for jm_0.1\n",
      "Reading results for jm_0.9\n",
      "Reading results for abs_0.9\n",
      "Reading results for dir_1500\n",
      "Reading results for dir_500\n",
      "Reading results for abs_0.1\n",
      "Reading results for abs_0.5\n",
      "Filtering rows on: map_cut_1000, ndcg_cut_10, P_5, recall_1000\n",
      "\n",
      "\n",
      "means: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>jm_0.5</th>\n",
       "      <th>dir_1000</th>\n",
       "      <th>jm_0.1</th>\n",
       "      <th>jm_0.9</th>\n",
       "      <th>abs_0.9</th>\n",
       "      <th>dir_1500</th>\n",
       "      <th>dir_500</th>\n",
       "      <th>abs_0.1</th>\n",
       "      <th>abs_0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.436975</td>\n",
       "      <td>0.448739</td>\n",
       "      <td>0.381513</td>\n",
       "      <td>0.475630</td>\n",
       "      <td>0.465546</td>\n",
       "      <td>0.452101</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.478992</td>\n",
       "      <td>0.472269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.395798</td>\n",
       "      <td>0.393496</td>\n",
       "      <td>0.379089</td>\n",
       "      <td>0.406345</td>\n",
       "      <td>0.399619</td>\n",
       "      <td>0.394598</td>\n",
       "      <td>0.390915</td>\n",
       "      <td>0.402112</td>\n",
       "      <td>0.402178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.440366</td>\n",
       "      <td>0.446853</td>\n",
       "      <td>0.394701</td>\n",
       "      <td>0.467395</td>\n",
       "      <td>0.459223</td>\n",
       "      <td>0.449911</td>\n",
       "      <td>0.442294</td>\n",
       "      <td>0.452233</td>\n",
       "      <td>0.461867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>0.982648</td>\n",
       "      <td>0.982648</td>\n",
       "      <td>0.982648</td>\n",
       "      <td>0.982648</td>\n",
       "      <td>0.982648</td>\n",
       "      <td>0.982648</td>\n",
       "      <td>0.982648</td>\n",
       "      <td>0.982648</td>\n",
       "      <td>0.982648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         metric    jm_0.5  dir_1000    jm_0.1    jm_0.9   abs_0.9  dir_1500  \\\n",
       "0           P_5  0.436975  0.448739  0.381513  0.475630  0.465546  0.452101   \n",
       "1  map_cut_1000  0.395798  0.393496  0.379089  0.406345  0.399619  0.394598   \n",
       "2   ndcg_cut_10  0.440366  0.446853  0.394701  0.467395  0.459223  0.449911   \n",
       "3   recall_1000  0.982648  0.982648  0.982648  0.982648  0.982648  0.982648   \n",
       "\n",
       "    dir_500   abs_0.1   abs_0.5  \n",
       "0  0.457143  0.478992  0.472269  \n",
       "1  0.390915  0.402112  0.402178  \n",
       "2  0.442294  0.452233  0.461867  \n",
       "3  0.982648  0.982648  0.982648  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvalues: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>pvalues</th>\n",
       "      <th>retrieval_pairs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.401676</td>\n",
       "      <td>jm_0.5_dir_1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>jm_0.5_jm_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.119581</td>\n",
       "      <td>jm_0.5_abs_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.307051</td>\n",
       "      <td>jm_0.5_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.083253</td>\n",
       "      <td>jm_0.5_dir_500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.064346</td>\n",
       "      <td>jm_0.5_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.079005</td>\n",
       "      <td>jm_0.5_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.375227</td>\n",
       "      <td>dir_1000_abs_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.222135</td>\n",
       "      <td>dir_1000_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.281951</td>\n",
       "      <td>dir_1000_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>jm_0.1_dir_1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>jm_0.1_abs_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>jm_0.1_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>jm_0.1_dir_500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>jm_0.1_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>jm_0.1_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>jm_0.9_jm_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.134609</td>\n",
       "      <td>jm_0.9_dir_1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>jm_0.9_jm_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.469178</td>\n",
       "      <td>jm_0.9_abs_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.210033</td>\n",
       "      <td>jm_0.9_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.223095</td>\n",
       "      <td>jm_0.9_dir_500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.819653</td>\n",
       "      <td>jm_0.9_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.790538</td>\n",
       "      <td>jm_0.9_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.452016</td>\n",
       "      <td>abs_0.9_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.601524</td>\n",
       "      <td>abs_0.9_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.529356</td>\n",
       "      <td>dir_1500_dir_1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.485217</td>\n",
       "      <td>dir_1500_abs_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.293346</td>\n",
       "      <td>dir_1500_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.375965</td>\n",
       "      <td>dir_1500_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.183592</td>\n",
       "      <td>jm_0.5_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.194885</td>\n",
       "      <td>dir_1000_abs_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.169931</td>\n",
       "      <td>dir_1000_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.125454</td>\n",
       "      <td>dir_1000_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>jm_0.1_dir_1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>jm_0.1_abs_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>jm_0.1_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.012855</td>\n",
       "      <td>jm_0.1_dir_500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>jm_0.1_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>jm_0.1_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>jm_0.9_jm_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>jm_0.9_dir_1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>jm_0.9_jm_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.037906</td>\n",
       "      <td>jm_0.9_abs_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.021604</td>\n",
       "      <td>jm_0.9_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>jm_0.9_dir_500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.154597</td>\n",
       "      <td>jm_0.9_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.112814</td>\n",
       "      <td>jm_0.9_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.479177</td>\n",
       "      <td>abs_0.9_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>abs_0.9_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.267510</td>\n",
       "      <td>dir_1500_dir_1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.334396</td>\n",
       "      <td>dir_1500_abs_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.266479</td>\n",
       "      <td>dir_1500_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.219882</td>\n",
       "      <td>dir_1500_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.101141</td>\n",
       "      <td>dir_500_dir_1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.037485</td>\n",
       "      <td>dir_500_abs_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.114673</td>\n",
       "      <td>dir_500_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.042651</td>\n",
       "      <td>dir_500_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.022534</td>\n",
       "      <td>dir_500_abs_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.962791</td>\n",
       "      <td>abs_0.5_abs_0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric   pvalues    retrieval_pairs\n",
       "0             P_5  0.401676    jm_0.5_dir_1000\n",
       "1             P_5  0.000561      jm_0.5_jm_0.1\n",
       "2             P_5  0.119581     jm_0.5_abs_0.9\n",
       "3             P_5  0.307051    jm_0.5_dir_1500\n",
       "4             P_5  0.083253     jm_0.5_dir_500\n",
       "5             P_5  0.064346     jm_0.5_abs_0.1\n",
       "6             P_5  0.079005     jm_0.5_abs_0.5\n",
       "7             P_5  0.375227   dir_1000_abs_0.9\n",
       "8             P_5  0.222135   dir_1000_abs_0.1\n",
       "9             P_5  0.281951   dir_1000_abs_0.5\n",
       "10            P_5  0.000689    jm_0.1_dir_1000\n",
       "11            P_5  0.000328     jm_0.1_abs_0.9\n",
       "12            P_5  0.000353    jm_0.1_dir_1500\n",
       "13            P_5  0.000143     jm_0.1_dir_500\n",
       "14            P_5  0.000517     jm_0.1_abs_0.1\n",
       "15            P_5  0.000312     jm_0.1_abs_0.5\n",
       "16            P_5  0.010014      jm_0.9_jm_0.5\n",
       "17            P_5  0.134609    jm_0.9_dir_1000\n",
       "18            P_5  0.000038      jm_0.9_jm_0.1\n",
       "19            P_5  0.469178     jm_0.9_abs_0.9\n",
       "20            P_5  0.210033    jm_0.9_dir_1500\n",
       "21            P_5  0.223095     jm_0.9_dir_500\n",
       "22            P_5  0.819653     jm_0.9_abs_0.1\n",
       "23            P_5  0.790538     jm_0.9_abs_0.5\n",
       "24            P_5  0.452016    abs_0.9_abs_0.1\n",
       "25            P_5  0.601524    abs_0.9_abs_0.5\n",
       "26            P_5  0.529356  dir_1500_dir_1000\n",
       "27            P_5  0.485217   dir_1500_abs_0.9\n",
       "28            P_5  0.293346   dir_1500_abs_0.1\n",
       "29            P_5  0.375965   dir_1500_abs_0.5\n",
       "..            ...       ...                ...\n",
       "114  map_cut_1000  0.183592     jm_0.5_abs_0.5\n",
       "115  map_cut_1000  0.194885   dir_1000_abs_0.9\n",
       "116  map_cut_1000  0.169931   dir_1000_abs_0.1\n",
       "117  map_cut_1000  0.125454   dir_1000_abs_0.5\n",
       "118  map_cut_1000  0.001791    jm_0.1_dir_1000\n",
       "119  map_cut_1000  0.001015     jm_0.1_abs_0.9\n",
       "120  map_cut_1000  0.000480    jm_0.1_dir_1500\n",
       "121  map_cut_1000  0.012855     jm_0.1_dir_500\n",
       "122  map_cut_1000  0.002507     jm_0.1_abs_0.1\n",
       "123  map_cut_1000  0.001202     jm_0.1_abs_0.5\n",
       "124  map_cut_1000  0.001231      jm_0.9_jm_0.5\n",
       "125  map_cut_1000  0.006667    jm_0.9_dir_1000\n",
       "126  map_cut_1000  0.000010      jm_0.9_jm_0.1\n",
       "127  map_cut_1000  0.037906     jm_0.9_abs_0.9\n",
       "128  map_cut_1000  0.021604    jm_0.9_dir_1500\n",
       "129  map_cut_1000  0.000203     jm_0.9_dir_500\n",
       "130  map_cut_1000  0.154597     jm_0.9_abs_0.1\n",
       "131  map_cut_1000  0.112814     jm_0.9_abs_0.5\n",
       "132  map_cut_1000  0.479177    abs_0.9_abs_0.1\n",
       "133  map_cut_1000  0.269000    abs_0.9_abs_0.5\n",
       "134  map_cut_1000  0.267510  dir_1500_dir_1000\n",
       "135  map_cut_1000  0.334396   dir_1500_abs_0.9\n",
       "136  map_cut_1000  0.266479   dir_1500_abs_0.1\n",
       "137  map_cut_1000  0.219882   dir_1500_abs_0.5\n",
       "138  map_cut_1000  0.101141   dir_500_dir_1000\n",
       "139  map_cut_1000  0.037485    dir_500_abs_0.9\n",
       "140  map_cut_1000  0.114673   dir_500_dir_1500\n",
       "141  map_cut_1000  0.042651    dir_500_abs_0.1\n",
       "142  map_cut_1000  0.022534    dir_500_abs_0.5\n",
       "143  map_cut_1000  0.962791    abs_0.5_abs_0.1\n",
       "\n",
       "[144 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>query_id</th>\n",
       "      <th>jm_0.5</th>\n",
       "      <th>dir_1000</th>\n",
       "      <th>jm_0.1</th>\n",
       "      <th>jm_0.9</th>\n",
       "      <th>abs_0.9</th>\n",
       "      <th>dir_1500</th>\n",
       "      <th>dir_500</th>\n",
       "      <th>abs_0.1</th>\n",
       "      <th>abs_0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.7001</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>0.5331</td>\n",
       "      <td>0.7105</td>\n",
       "      <td>0.8263</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>0.7001</td>\n",
       "      <td>0.7163</td>\n",
       "      <td>0.7799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>101</td>\n",
       "      <td>0.5837</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.6060</td>\n",
       "      <td>0.6033</td>\n",
       "      <td>0.4506</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.4787</td>\n",
       "      <td>0.5033</td>\n",
       "      <td>0.4734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>102</td>\n",
       "      <td>0.5022</td>\n",
       "      <td>0.5587</td>\n",
       "      <td>0.4915</td>\n",
       "      <td>0.4983</td>\n",
       "      <td>0.5072</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>0.5022</td>\n",
       "      <td>0.3594</td>\n",
       "      <td>0.4761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>104</td>\n",
       "      <td>0.4473</td>\n",
       "      <td>0.2980</td>\n",
       "      <td>0.7382</td>\n",
       "      <td>0.2934</td>\n",
       "      <td>0.2122</td>\n",
       "      <td>0.3878</td>\n",
       "      <td>0.3382</td>\n",
       "      <td>0.2903</td>\n",
       "      <td>0.2173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>105</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>106</td>\n",
       "      <td>0.5617</td>\n",
       "      <td>0.6497</td>\n",
       "      <td>0.4883</td>\n",
       "      <td>0.5252</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4558</td>\n",
       "      <td>0.3536</td>\n",
       "      <td>0.3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>107</td>\n",
       "      <td>0.4955</td>\n",
       "      <td>0.5965</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.7846</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.5919</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>0.7968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>108</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.1834</td>\n",
       "      <td>0.1447</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.2048</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.2173</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.2489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>109</td>\n",
       "      <td>0.4681</td>\n",
       "      <td>0.6169</td>\n",
       "      <td>0.4681</td>\n",
       "      <td>0.4681</td>\n",
       "      <td>0.5694</td>\n",
       "      <td>0.6169</td>\n",
       "      <td>0.6139</td>\n",
       "      <td>0.5443</td>\n",
       "      <td>0.5443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>110</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.8263</td>\n",
       "      <td>0.5463</td>\n",
       "      <td>0.4131</td>\n",
       "      <td>0.5549</td>\n",
       "      <td>0.8205</td>\n",
       "      <td>0.4787</td>\n",
       "      <td>0.4964</td>\n",
       "      <td>0.5112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>112</td>\n",
       "      <td>0.4153</td>\n",
       "      <td>0.5384</td>\n",
       "      <td>0.4252</td>\n",
       "      <td>0.4085</td>\n",
       "      <td>0.3301</td>\n",
       "      <td>0.5384</td>\n",
       "      <td>0.4885</td>\n",
       "      <td>0.3301</td>\n",
       "      <td>0.3301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>115</td>\n",
       "      <td>0.6379</td>\n",
       "      <td>0.4124</td>\n",
       "      <td>0.4171</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.3038</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>0.5521</td>\n",
       "      <td>0.1396</td>\n",
       "      <td>0.2064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>116</td>\n",
       "      <td>0.3128</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.2686</td>\n",
       "      <td>0.3281</td>\n",
       "      <td>0.4221</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1584</td>\n",
       "      <td>0.4288</td>\n",
       "      <td>0.4288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>117</td>\n",
       "      <td>0.3183</td>\n",
       "      <td>0.3183</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>0.2248</td>\n",
       "      <td>0.2330</td>\n",
       "      <td>0.3183</td>\n",
       "      <td>0.3183</td>\n",
       "      <td>0.2115</td>\n",
       "      <td>0.2248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>118</td>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.4506</td>\n",
       "      <td>0.3133</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.2782</td>\n",
       "      <td>0.3693</td>\n",
       "      <td>0.3633</td>\n",
       "      <td>0.3430</td>\n",
       "      <td>0.3348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>119</td>\n",
       "      <td>0.5876</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.4233</td>\n",
       "      <td>0.6055</td>\n",
       "      <td>0.6332</td>\n",
       "      <td>0.3421</td>\n",
       "      <td>0.5213</td>\n",
       "      <td>0.6749</td>\n",
       "      <td>0.6785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>122</td>\n",
       "      <td>0.2687</td>\n",
       "      <td>0.1795</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>0.2051</td>\n",
       "      <td>0.2457</td>\n",
       "      <td>0.2399</td>\n",
       "      <td>0.1799</td>\n",
       "      <td>0.1732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>124</td>\n",
       "      <td>0.7020</td>\n",
       "      <td>0.7020</td>\n",
       "      <td>0.6805</td>\n",
       "      <td>0.6870</td>\n",
       "      <td>0.6722</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.7020</td>\n",
       "      <td>0.6652</td>\n",
       "      <td>0.5984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>125</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.9306</td>\n",
       "      <td>0.8358</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9306</td>\n",
       "      <td>0.9337</td>\n",
       "      <td>0.9337</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric query_id  jm_0.5  dir_1000  jm_0.1  jm_0.9  abs_0.9  \\\n",
       "56    ndcg_cut_10      100  0.7001    0.6727  0.5331  0.7105   0.8263   \n",
       "147   ndcg_cut_10      101  0.5837    0.4548  0.6060  0.6033   0.4506   \n",
       "238   ndcg_cut_10      102  0.5022    0.5587  0.4915  0.4983   0.5072   \n",
       "329   ndcg_cut_10      104  0.4473    0.2980  0.7382  0.2934   0.2122   \n",
       "420   ndcg_cut_10      105  0.0000    0.0000  0.0000  0.0000   0.0000   \n",
       "511   ndcg_cut_10      106  0.5617    0.6497  0.4883  0.5252   0.4115   \n",
       "602   ndcg_cut_10      107  0.4955    0.5965  0.2182  0.7846   0.7818   \n",
       "693   ndcg_cut_10      108  0.2083    0.1834  0.1447  0.2240   0.2048   \n",
       "784   ndcg_cut_10      109  0.4681    0.6169  0.4681  0.4681   0.5694   \n",
       "875   ndcg_cut_10      110  0.3700    0.8263  0.5463  0.4131   0.5549   \n",
       "966   ndcg_cut_10      112  0.4153    0.5384  0.4252  0.4085   0.3301   \n",
       "1057  ndcg_cut_10      115  0.6379    0.4124  0.4171  0.3130   0.3038   \n",
       "1148  ndcg_cut_10      116  0.3128    0.0636  0.2686  0.3281   0.4221   \n",
       "1239  ndcg_cut_10      117  0.3183    0.3183  0.6727  0.2248   0.2330   \n",
       "1330  ndcg_cut_10      118  0.4604    0.4506  0.3133  0.4066   0.2782   \n",
       "1421  ndcg_cut_10      119  0.5876    0.4115  0.4233  0.6055   0.6332   \n",
       "1512  ndcg_cut_10      121  0.0784    0.0000  0.0000  0.0694   0.0000   \n",
       "1603  ndcg_cut_10      122  0.2687    0.1795  0.0694  0.1642   0.2051   \n",
       "1694  ndcg_cut_10      124  0.7020    0.7020  0.6805  0.6870   0.6722   \n",
       "1785  ndcg_cut_10      125  0.9364    0.9306  0.8358  1.0000   0.9306   \n",
       "\n",
       "      dir_1500  dir_500  abs_0.1  abs_0.5  \n",
       "56      0.6727   0.7001   0.7163   0.7799  \n",
       "147     0.4548   0.4787   0.5033   0.4734  \n",
       "238     0.5619   0.5022   0.3594   0.4761  \n",
       "329     0.3878   0.3382   0.2903   0.2173  \n",
       "420     0.0000   0.0000   0.0000   0.0000  \n",
       "511     0.7543   0.4558   0.3536   0.3024  \n",
       "602     0.5919   0.6122   0.7273   0.7968  \n",
       "693     0.1732   0.2173   0.2240   0.2489  \n",
       "784     0.6169   0.6139   0.5443   0.5443  \n",
       "875     0.8205   0.4787   0.4964   0.5112  \n",
       "966     0.5384   0.4885   0.3301   0.3301  \n",
       "1057    0.3922   0.5521   0.1396   0.2064  \n",
       "1148    0.0000   0.1584   0.4288   0.4288  \n",
       "1239    0.3183   0.3183   0.2115   0.2248  \n",
       "1330    0.3693   0.3633   0.3430   0.3348  \n",
       "1421    0.3421   0.5213   0.6749   0.6785  \n",
       "1512    0.0000   0.0000   0.0663   0.0000  \n",
       "1603    0.2457   0.2399   0.1799   0.1732  \n",
       "1694    0.6988   0.7020   0.6652   0.5984  \n",
       "1785    0.9337   0.9337   1.0000   0.9364  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use these for tuning hyper parameters\n",
    "fpath_qrel_val = './ap_88_89/qrel_validation'\n",
    "output_dir_val = \"results_val\"\n",
    "\n",
    "retrieval_models = { \n",
    "    \n",
    "    # Jelinek-Mercer\n",
    "    'jm_0.1' : lambda x,y : jelinek_mercer(x, y, lambda_ = 0.1),\n",
    "    'jm_0.5' : lambda x,y : jelinek_mercer(x, y, lambda_ = 0.5),\n",
    "    'jm_0.9' : lambda x,y : jelinek_mercer(x, y, lambda_ = 0.9),\n",
    "\n",
    "    # Dirichlet prior\n",
    "    'dir_500'  : lambda x,y : dirichlet_prior(x, y, mu = 500),\n",
    "    'dir_1000' : lambda x,y : dirichlet_prior(x, y, mu = 1000),\n",
    "    'dir_1500' : lambda x,y : dirichlet_prior(x, y, mu = 1500),\n",
    "    \n",
    "    # absolute discounting\n",
    "    'abs_0.1' : lambda x,y : absolute_discounting(x, y, delta = 0.1),\n",
    "    'abs_0.5' : lambda x,y : absolute_discounting(x, y, delta = 0.5),\n",
    "    'abs_0.9' : lambda x,y : absolute_discounting(x, y, delta = 0.9)\n",
    "}\n",
    "\n",
    "metrics = [ 'map_cut_1000', 'ndcg_cut_10', 'P_5', 'recall_1000' ]\n",
    "\n",
    "clean_evaluation_results(retrieval_models, output_dirs=[output_dir_test])\n",
    "\n",
    "generate_evaluation_results(retrieval_models, fpath_qrel_test, output_dir_test)  \n",
    "\n",
    "result_dir = \"results_eval_lm\"\n",
    "df_results = build_df_results(retrieval_models, metrics, output_dir_test)\n",
    "df_results, df_means, df_pvalues = analyze_evaluation_results(df_results, print_df=True, result_dir = result_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use these for tuning hyper parameters\n",
    "fpath_qrel_val = './ap_88_89/qrel_validation'\n",
    "output_dir_val = \"results_val\"\n",
    "\n",
    "# build dict with retrieval functions\n",
    "# for all combinations of kernel functions\n",
    "# and mu hyper parameters in { 500, 1000, 1500 }\n",
    "retrieval_models = { }\n",
    "\n",
    "kernels = {\n",
    "    \"gaussian\" : gaussian_kernel, \n",
    "    \"circle\"   : circle_kernel, \n",
    "    \"triangle\" : triangle_kernel, \n",
    "    \"passage\"  : passage_kernel, \n",
    "    \"cosine\"   : cosine_kernel\n",
    "}\n",
    "\n",
    "def add_plm_models(kname, fn_k, mu_values):\n",
    "    zi_cache = {} # can be shared by models with the same kernel (but different mu)\n",
    "    for mu in mu_values: # loop over mu values\n",
    "        retrieval_models[\"plm_{}_{}\".format(kname, mu)] = lambda x,y: positional_language_model(\n",
    "            x, y, inverted_index_positions, fn_k, zi_cache)\n",
    "\n",
    "for kname, fn_k in kernels.items(): # loop over kernel functions\n",
    "    add_plm_models(kname, fn_k, [ 500, 1000, 1500 ])\n",
    "    \n",
    "\n",
    "metrics = [ 'map_cut_1000', 'ndcg_cut_10', 'P_5', 'recall_1000' ]\n",
    "\n",
    "clean_evaluation_results(retrieval_models, output_dirs=[output_dir_test])\n",
    "\n",
    "generate_evaluation_results(retrieval_models, fpath_qrel_test, output_dir_test)  \n",
    "\n",
    "result_dir = \"results_eval_plm\"\n",
    "df_results = build_df_results(retrieval_models, metrics, output_dir_test)\n",
    "df_results, df_means, df_pvalues = analyze_evaluation_results(df_results, print_df=True, result_dir = result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting results_test/dir_1500.txt if exists\n",
      "Deleting results_test/BM25.txt if exists\n",
      "Deleting results_test/abs_0.1.txt if exists\n",
      "Deleting results_test/jm_0.9.txt if exists\n",
      "Deleting results_test/tfidf.txt if exists\n",
      "\n",
      "Retrieving using dir_1500\n",
      "retrieval for 'dir_1500' took 2.57 secondes: \n",
      "Retrieving using BM25\n",
      "retrieval for 'BM25' took 2.14 secondes: \n",
      "Retrieving using abs_0.1\n",
      "retrieval for 'abs_0.1' took 6.40 secondes: \n",
      "Retrieving using jm_0.9\n",
      "retrieval for 'jm_0.9' took 2.48 secondes: \n",
      "Retrieving using tfidf\n",
      "retrieval for 'tfidf' took 1.12 secondes: \n",
      "\n",
      "Generating results for dir_1500\n",
      "Generating results for BM25\n",
      "Generating results for abs_0.1\n",
      "Generating results for jm_0.9\n",
      "Generating results for tfidf\n",
      "\n",
      "Reading results for dir_1500\n",
      "Reading results for BM25\n",
      "Reading results for abs_0.1\n",
      "Reading results for jm_0.9\n",
      "Reading results for tfidf\n",
      "Filtering rows on: map_cut_1000, ndcg_cut_10, P_5, recall_1000\n",
      "\n",
      "\n",
      "means: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>dir_1500</th>\n",
       "      <th>BM25</th>\n",
       "      <th>abs_0.1</th>\n",
       "      <th>jm_0.9</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.452101</td>\n",
       "      <td>0.467227</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.475630</td>\n",
       "      <td>0.504202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.394598</td>\n",
       "      <td>0.407674</td>\n",
       "      <td>0.400838</td>\n",
       "      <td>0.406353</td>\n",
       "      <td>0.409892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.449911</td>\n",
       "      <td>0.479071</td>\n",
       "      <td>0.459361</td>\n",
       "      <td>0.467395</td>\n",
       "      <td>0.493584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>0.982648</td>\n",
       "      <td>0.982648</td>\n",
       "      <td>0.982648</td>\n",
       "      <td>0.982648</td>\n",
       "      <td>0.982648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         metric  dir_1500      BM25   abs_0.1    jm_0.9     tfidf\n",
       "0           P_5  0.452101  0.467227  0.470588  0.475630  0.504202\n",
       "1  map_cut_1000  0.394598  0.407674  0.400838  0.406353  0.409892\n",
       "2   ndcg_cut_10  0.449911  0.479071  0.459361  0.467395  0.493584\n",
       "3   recall_1000  0.982648  0.982648  0.982648  0.982648  0.982648"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvalues: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>pvalues</th>\n",
       "      <th>retrieval_pairs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.363013</td>\n",
       "      <td>dir_1500_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.410638</td>\n",
       "      <td>dir_1500_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.848330</td>\n",
       "      <td>abs_0.1_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.210033</td>\n",
       "      <td>jm_0.9_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.555138</td>\n",
       "      <td>jm_0.9_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.682081</td>\n",
       "      <td>jm_0.9_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.011042</td>\n",
       "      <td>tfidf_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.047751</td>\n",
       "      <td>tfidf_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.145398</td>\n",
       "      <td>tfidf_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>tfidf_jm_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dir_1500_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dir_1500_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abs_0.1_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jm_0.9_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jm_0.9_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jm_0.9_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfidf_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfidf_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfidf_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfidf_jm_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.022219</td>\n",
       "      <td>dir_1500_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.557765</td>\n",
       "      <td>dir_1500_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.124862</td>\n",
       "      <td>abs_0.1_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.216011</td>\n",
       "      <td>jm_0.9_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.262468</td>\n",
       "      <td>jm_0.9_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.271074</td>\n",
       "      <td>jm_0.9_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>tfidf_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.333338</td>\n",
       "      <td>tfidf_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.057297</td>\n",
       "      <td>tfidf_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.130745</td>\n",
       "      <td>tfidf_jm_0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>dir_1500_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.310295</td>\n",
       "      <td>dir_1500_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.142317</td>\n",
       "      <td>abs_0.1_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.021530</td>\n",
       "      <td>jm_0.9_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.693646</td>\n",
       "      <td>jm_0.9_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.047263</td>\n",
       "      <td>jm_0.9_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>tfidf_dir_1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.601971</td>\n",
       "      <td>tfidf_BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.163434</td>\n",
       "      <td>tfidf_abs_0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>map_cut_1000</td>\n",
       "      <td>0.517130</td>\n",
       "      <td>tfidf_jm_0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          metric   pvalues   retrieval_pairs\n",
       "0            P_5  0.363013     dir_1500_BM25\n",
       "1            P_5  0.410638  dir_1500_abs_0.1\n",
       "2            P_5  0.848330      abs_0.1_BM25\n",
       "3            P_5  0.210033   jm_0.9_dir_1500\n",
       "4            P_5  0.555138       jm_0.9_BM25\n",
       "5            P_5  0.682081    jm_0.9_abs_0.1\n",
       "6            P_5  0.011042    tfidf_dir_1500\n",
       "7            P_5  0.047751        tfidf_BM25\n",
       "8            P_5  0.145398     tfidf_abs_0.1\n",
       "9            P_5  0.167500      tfidf_jm_0.9\n",
       "10   recall_1000       NaN     dir_1500_BM25\n",
       "11   recall_1000       NaN  dir_1500_abs_0.1\n",
       "12   recall_1000       NaN      abs_0.1_BM25\n",
       "13   recall_1000       NaN   jm_0.9_dir_1500\n",
       "14   recall_1000       NaN       jm_0.9_BM25\n",
       "15   recall_1000       NaN    jm_0.9_abs_0.1\n",
       "16   recall_1000       NaN    tfidf_dir_1500\n",
       "17   recall_1000       NaN        tfidf_BM25\n",
       "18   recall_1000       NaN     tfidf_abs_0.1\n",
       "19   recall_1000       NaN      tfidf_jm_0.9\n",
       "20   ndcg_cut_10  0.022219     dir_1500_BM25\n",
       "21   ndcg_cut_10  0.557765  dir_1500_abs_0.1\n",
       "22   ndcg_cut_10  0.124862      abs_0.1_BM25\n",
       "23   ndcg_cut_10  0.216011   jm_0.9_dir_1500\n",
       "24   ndcg_cut_10  0.262468       jm_0.9_BM25\n",
       "25   ndcg_cut_10  0.271074    jm_0.9_abs_0.1\n",
       "26   ndcg_cut_10  0.003909    tfidf_dir_1500\n",
       "27   ndcg_cut_10  0.333338        tfidf_BM25\n",
       "28   ndcg_cut_10  0.057297     tfidf_abs_0.1\n",
       "29   ndcg_cut_10  0.130745      tfidf_jm_0.9\n",
       "30  map_cut_1000  0.004219     dir_1500_BM25\n",
       "31  map_cut_1000  0.310295  dir_1500_abs_0.1\n",
       "32  map_cut_1000  0.142317      abs_0.1_BM25\n",
       "33  map_cut_1000  0.021530   jm_0.9_dir_1500\n",
       "34  map_cut_1000  0.693646       jm_0.9_BM25\n",
       "35  map_cut_1000  0.047263    jm_0.9_abs_0.1\n",
       "36  map_cut_1000  0.003357    tfidf_dir_1500\n",
       "37  map_cut_1000  0.601971        tfidf_BM25\n",
       "38  map_cut_1000  0.163434     tfidf_abs_0.1\n",
       "39  map_cut_1000  0.517130      tfidf_jm_0.9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>query_id</th>\n",
       "      <th>dir_1500</th>\n",
       "      <th>BM25</th>\n",
       "      <th>abs_0.1</th>\n",
       "      <th>jm_0.9</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>0.7289</td>\n",
       "      <td>0.7799</td>\n",
       "      <td>0.7105</td>\n",
       "      <td>0.7506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>101</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.4734</td>\n",
       "      <td>0.6033</td>\n",
       "      <td>0.1732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>102</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>0.4925</td>\n",
       "      <td>0.4761</td>\n",
       "      <td>0.4983</td>\n",
       "      <td>0.4698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>104</td>\n",
       "      <td>0.3878</td>\n",
       "      <td>0.4658</td>\n",
       "      <td>0.2173</td>\n",
       "      <td>0.2934</td>\n",
       "      <td>0.2992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>105</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>106</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.6118</td>\n",
       "      <td>0.3024</td>\n",
       "      <td>0.5252</td>\n",
       "      <td>0.5829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>107</td>\n",
       "      <td>0.5919</td>\n",
       "      <td>0.8007</td>\n",
       "      <td>0.7968</td>\n",
       "      <td>0.7846</td>\n",
       "      <td>0.7034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>108</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.1834</td>\n",
       "      <td>0.2489</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.2497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>109</td>\n",
       "      <td>0.6169</td>\n",
       "      <td>0.4681</td>\n",
       "      <td>0.5443</td>\n",
       "      <td>0.4681</td>\n",
       "      <td>0.4570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>110</td>\n",
       "      <td>0.8205</td>\n",
       "      <td>0.5420</td>\n",
       "      <td>0.5112</td>\n",
       "      <td>0.4131</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>112</td>\n",
       "      <td>0.5384</td>\n",
       "      <td>0.5017</td>\n",
       "      <td>0.3301</td>\n",
       "      <td>0.4085</td>\n",
       "      <td>0.6086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>115</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>0.4518</td>\n",
       "      <td>0.2064</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.1514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>116</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1763</td>\n",
       "      <td>0.4288</td>\n",
       "      <td>0.3281</td>\n",
       "      <td>0.0663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>117</td>\n",
       "      <td>0.3183</td>\n",
       "      <td>0.3945</td>\n",
       "      <td>0.2248</td>\n",
       "      <td>0.2248</td>\n",
       "      <td>0.6253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>118</td>\n",
       "      <td>0.3693</td>\n",
       "      <td>0.4951</td>\n",
       "      <td>0.3348</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.2900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>119</td>\n",
       "      <td>0.3421</td>\n",
       "      <td>0.6431</td>\n",
       "      <td>0.6785</td>\n",
       "      <td>0.6055</td>\n",
       "      <td>0.3169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>122</td>\n",
       "      <td>0.2457</td>\n",
       "      <td>0.2863</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>0.2051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>124</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.6831</td>\n",
       "      <td>0.5984</td>\n",
       "      <td>0.6870</td>\n",
       "      <td>0.6911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>125</td>\n",
       "      <td>0.9337</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric query_id  dir_1500    BM25  abs_0.1  jm_0.9   tfidf\n",
       "56    ndcg_cut_10      100    0.6727  0.7289   0.7799  0.7105  0.7506\n",
       "147   ndcg_cut_10      101    0.4548  0.5303   0.4734  0.6033  0.1732\n",
       "238   ndcg_cut_10      102    0.5619  0.4925   0.4761  0.4983  0.4698\n",
       "329   ndcg_cut_10      104    0.3878  0.4658   0.2173  0.2934  0.2992\n",
       "420   ndcg_cut_10      105    0.0000  0.0000   0.0000  0.0000  0.0000\n",
       "511   ndcg_cut_10      106    0.7543  0.6118   0.3024  0.5252  0.5829\n",
       "602   ndcg_cut_10      107    0.5919  0.8007   0.7968  0.7846  0.7034\n",
       "693   ndcg_cut_10      108    0.1732  0.1834   0.2489  0.2240  0.2497\n",
       "784   ndcg_cut_10      109    0.6169  0.4681   0.5443  0.4681  0.4570\n",
       "875   ndcg_cut_10      110    0.8205  0.5420   0.5112  0.4131  1.0000\n",
       "966   ndcg_cut_10      112    0.5384  0.5017   0.3301  0.4085  0.6086\n",
       "1057  ndcg_cut_10      115    0.3922  0.4518   0.2064  0.3130  0.1514\n",
       "1148  ndcg_cut_10      116    0.0000  0.1763   0.4288  0.3281  0.0663\n",
       "1239  ndcg_cut_10      117    0.3183  0.3945   0.2248  0.2248  0.6253\n",
       "1330  ndcg_cut_10      118    0.3693  0.4951   0.3348  0.4066  0.2900\n",
       "1421  ndcg_cut_10      119    0.3421  0.6431   0.6785  0.6055  0.3169\n",
       "1512  ndcg_cut_10      121    0.0000  0.0000   0.0000  0.0694  0.0000\n",
       "1603  ndcg_cut_10      122    0.2457  0.2863   0.1732  0.1642  0.2051\n",
       "1694  ndcg_cut_10      124    0.6988  0.6831   0.5984  0.6870  0.6911\n",
       "1785  ndcg_cut_10      125    0.9337  0.9364   0.9364  1.0000  0.5518"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use these for camparison of metrics\n",
    "fpath_qrel_test = './ap_88_89/qrel_test'\n",
    "output_dir_test = \"results_test\"\n",
    "\n",
    "cache_zi_gaussian_50 = {}\n",
    "retrieval_models = { \n",
    "    'tfidf' : tfidf, \n",
    "    'BM25': BM25,\n",
    "    'jm_0.9' : lambda x,y : jelinek_mercer(x, y, 0.9),\n",
    "    'dir_1500' : lambda x,y : dirichlet_prior(x, y, 1500), \n",
    "    'abs_0.1' : lambda x,y : absolute_discounting(x, y, 0.5),\n",
    "#     'plm_gaussian_x' : lambda x,y :positional_language_model(x,y, inverted_index_positions, gaussian_kernel, cache_zi_gaussian_50)\n",
    "}\n",
    "\n",
    "metrics = [ 'map_cut_1000', 'ndcg_cut_10', 'P_5', 'recall_1000' ]\n",
    "\n",
    "clean_evaluation_results(retrieval_models, output_dirs=[output_dir_test])\n",
    "\n",
    "generate_evaluation_results(retrieval_models, fpath_qrel_test, output_dir_test)  \n",
    "\n",
    "result_dir = \"results_eval_test\"\n",
    "df_results = build_df_results(retrieval_models, metrics, output_dir_test)\n",
    "df_results, df_means, df_pvalues = analyze_evaluation_results(df_results, print_df=True, result_dir = result_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging, gensim, bz2\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.corpora.lowcorpus import LowCorpus\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "dictionary_obj = gensim.corpora.dictionary.Dictionary()\n",
    "dictionary_obj.token2id = token2id\n",
    "\n",
    "# example\n",
    "dictionary_obj.doc2idx(document=['human', 'computer', 'interface'], unknown_word_index=-1)\n",
    "print(dictionary_obj)\n",
    "\n",
    "\n",
    "#TODO: CREATE CORPUS\n",
    "#MmCorpus.serialize('test.mm', test_corpus)\n",
    "#mm = MmCorpus('test.mm') # `mm` document stream now has random access\n",
    "#print(mm[42]) # retrieve document no. 42, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODELS (both need corpus)\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# extract 400 LSI topics; use the default one-pass algorithm\n",
    "#lsi = LsiModel(corpus=test_corpus, id2word=id2token, num_topics=10)\n",
    "#print(lsi)\n",
    "\n",
    "# print the most contributing words (both positively and negatively) for each of the first ten topics\n",
    "#lsi.print_topics(10)\n",
    "\n",
    "#lda = LdaModel(corpus, num_topics=10)  # train model\n",
    "#print(lda[doc_bow]) # get topic probability distribution for a document\n",
    "#lda.update(corpus2) # update the LDA model with additional documents\n",
    "#print(lda[doc_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CLASS FROM PYNDRI WITH MINOR CHANGE\n",
    "class SelfMadeIndriSentences(gensim.interfaces.CorpusABC):\n",
    "    \"\"\"Integrates an Index with Gensim's word2vec implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, index, dictionary, max_documents=None):\n",
    "        assert isinstance(index, pyndri.Index)\n",
    "        self.index = index\n",
    "        self.dictionary = dictionary\n",
    "        self.max_documents = max_documents\n",
    "\n",
    "    def _maximum_document(self):\n",
    "        if self.max_documents is None:\n",
    "            return self.index.maximum_document()\n",
    "        else:\n",
    "            return min(\n",
    "                self.max_documents + self.index.document_base(),\n",
    "                self.index.maximum_document())\n",
    "\n",
    "    def __iter__(self):\n",
    "        for int_doc_id in range(self.index.document_base(),\n",
    "                                self._maximum_document()):\n",
    "            ext_doc_id, tokens = self.index.document(int_doc_id)\n",
    "\n",
    "            # CHANGED:\n",
    "            yield [token_id\n",
    "                for token_id in tokens\n",
    "                if token_id > 0 and token_id in self.dictionary]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._maximum_document() - self.index.document_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Should we apply a tfidf transformation to the corpus before\n",
    "applying LSI / LDA? Is it something we can decide freely on?\n",
    "Yes. It's up to you.\n",
    "------------------------------------------------------------\n",
    "ValueError: too many values to unpack (expected 2)\n",
    "That's expected. You need to implement a similar class, \n",
    "but one that returns the right representation of an item. \n",
    "IndriSentences returns strings, but you need a sparse vector of token ids.\n",
    "'''\n",
    "from pyndri import compat\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "sentences = SelfMadeIndriSentences(index, dictionary)\n",
    "print(sentences)\n",
    "print(sentences.index)\n",
    "print(sentences.dictionary)\n",
    "print(sentences.max_documents)\n",
    "print(len(sentences))\n",
    "#lsi = gensim.models.lsimodel.LsiModel(corpus=sentences, id2word=dictionary.id2token, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sen in sentences:\n",
    "    print(sen)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = gensim.models.lsimodel.LsiModel(corpus=sentences,\n",
    "        id2word=dictionary.id2token, num_topics=10)\n",
    "print(lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return 1 - spatial.distance.cosine(vec1, vec2)\n",
    "\n",
    "print(cosine_similarity([0, 0, 1, 1], [0, 0, 1, 1]))\n",
    "print(cosine_similarity([0, 0, 1, 1], [1, 1, 0, 0]))\n",
    "print(cosine_similarity([0, 0, 1, 1], [1, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from pyndri import compat\n",
    "import copy\n",
    "\n",
    "def word2vec(epochs = 1):\n",
    "    \n",
    "    word2vec_init = gensim.models.Word2Vec(\n",
    "        size=300,    # Embedding size\n",
    "        window=5,    # One-sided window size\n",
    "        sg=True,     # Skip-gram.\n",
    "        min_count=5, # Minimum word frequency.\n",
    "        sample=1e-3, # Sub-sample threshold.\n",
    "        hs=False,    # Hierarchical softmax.\n",
    "        negative=10, # Number of negative examples.\n",
    "        iter=1,      # Number of iterations.\n",
    "        workers=8,   # Number of workers.\n",
    "    )\n",
    "\n",
    "    with pyndri.open('./index') as index:\n",
    "        dictionary = pyndri.extract_dictionary(index)\n",
    "        sentences = compat.IndriSentences(index, dictionary)\n",
    "\n",
    "        # Build vocab\n",
    "        word2vec_init.build_vocab(sentences, trim_rule=None)\n",
    "\n",
    "        model = copy.deepcopy(word2vec_init)\n",
    "        model.train(sentences, total_examples = model.corpus_count, epochs = epochs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "# word_vectors = word2vec(epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = 'word_vectors'\n",
    "\n",
    "# Use to save model\n",
    "# word_vectors.save(fname) \n",
    "\n",
    "# Use to load model\n",
    "word_vectors = gensim.models.Word2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a word vector\n",
    "# word_vectors.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_weighted_vector(query):\n",
    "    query_tokens = [token for token in index.tokenize(query) if len(token) > 0]\n",
    "    query2vectors = word_vectors.wv[query_tokens]\n",
    "    \n",
    "    weighted_vectors = []\n",
    "    \n",
    "    for query_token, query_id_token in zip(query_tokens, [token2id.get(query_token,0) for query_token in query_tokens]):\n",
    "        corpus_term_freq = total_corpus_term_freqs[query_id_token]\n",
    "        word_vec = word_vectors.wv[query_token]\n",
    "        \n",
    "        weighted_vectors.append(word_vec / corpus_term_freq)\n",
    "        \n",
    "    return np.mean(weighted_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_vector = avg_weighted_vector(\"bitch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bitch', 1.0000001192092896),\n",
       " ('jokester', 0.6488711833953857),\n",
       " ('circuslike', 0.6444293260574341),\n",
       " ('colich', 0.6300046443939209),\n",
       " ('mondellos', 0.6262781023979187),\n",
       " ('nunally', 0.6249598264694214),\n",
       " ('smirking', 0.6212821006774902),\n",
       " ('aronne', 0.6179535984992981),\n",
       " ('ranted', 0.616774320602417),\n",
       " ('villane', 0.6164078712463379)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.wv.similar_by_vector(query_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 4: Write a report [15 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Only send us the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file structure as the one we have provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
