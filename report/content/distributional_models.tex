\section{Distributioal semantics}

In this section we compare different distributional semantics methods (Latent Semantic Models) for information retrieval.
(Section \ref{DistributionalSemantics:Models}) using
offline evaluation (Section \ref{DistributionalSemantics:Evaluation}).

\subsection{Models}
\label{DistributionalSemantics:Models}

\subsubsection{Latent Semantic Indexing (LSI)}
LSI maps a set of documents and terms into a set of concepts related to the documents and terms, using Singular Value Decomposition (SVD) to reduce the dimensions used in the original data. Therefore documents and terms are mapped to a latent space containing one \'topic\' per dimension and can be compared in terms of similarity by comparing the resulting embeddings. 

\subsubsection{Latent Dirichlet Allocation (LDA)}
% https://cs.stanford.edu/~ppasupat/a9online/1140.html
% http://www.indiciumweb.co.uk/2010/09/latent-dirichlet-allocation-v-latent-semantic-indexing/

\subsubsection{Implementing and training the models}
Both models are implemented with the \textit{gensim} library for python. Each of the two models is trained on a corpus consisting of sparse vector representations of documents. These consist of term-ids and their occurrences in the document. Furthermore, the model uses a dictionary with id-to-token mapping to map term-ids to actual words (tokens) and to determine the vocabulary size. Thirdly, the amount of topics (latent dimensions) was set to XXX after hyperparameter search.


% TODO: - find best value for num_topics
% TODO: "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices."
% LSI (hyper)parameters: corpus=None, num_topics=200, id2word=None, chunksize=20000, decay=1.0, distributed=False, onepass=True, power_iters=2, extra_samples=100, dtype=<type 'numpy.float64'>)
% LDA (hyper)parameters: corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf=None, minimum_phi_value=0.01, per_word_topics=False, callbacks=None, dtype=<type 'numpy.float32'>

Then, both models were trained for 10 epochs, and the trained models were used to map existing queries and documents to their corresponding latent space (either using LSI or LDA). The resulting embeddings of a query and document are compared using cosine similarity, which measures the angle between the embedding vectors. Another similarity measure, the KL divergence, was also tried but showed to be inappropriate for this task, since the resulting embeddings are no probability distributions and contain many zeros.

Using methods described above, a given set of documents for some query can be reranked by calculating the cosine similarity of each query-document embedding pair and sorting the documents based on cosine similarity. Finally, the reranked sets are evaluated using the previous methods (NDCG@10, Mean Average Precision (MAP@1000), Precision@5 and Recall@1000).




\subsection{Offline Evaluation}
\label{DistributionalSemantics:Evaluation}

\subsubsection{Experimental Setup}

\paragraph{Testset}
- XXX queries with an average of YY labeled documents per query.
labels: relevant or not relevant

- run our retrieval functions and score all labeled document-query pairs
this results in a ranking of the documents for the query

- run trec-eval to evaluate this ranking using different metrics


\paragraph{Metrics}
We use trec eval and compare on ...

- NDCG@10
- Mean Average Precision (MAP@1000) 
- Precision@5
- Recall@1000

\paragraph{Optimizing Hyper Parameters}
%- We use NDCG@10 as a metric. Why?
%\input{content/tbl_plm_hyperparams}


\subsubsection{Results}
%- means
%\input{content/tbl_means}
%- pvalues
%- manual inspection

\subsubsection{Analysis}
%- Do all methods perform similarly on all queries? Why?
%- Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?


Perform significance testing [5 points] (similar as in Task 1) in the class of semantic matching methods.