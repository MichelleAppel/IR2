\section{Lexical Models}

In this section we compare different lexical retrieval models
(Section \ref{LexicalModels:Models}) using
offline evaluation (Section \ref{LexicalModels:Evaluation}).

\subsection{Models}
\label{LexicalModels:Models}

%%%%%%%%% It should explain what you have implemented

\subsubsection{TF-IDF}

The tf\textendash idf score is the product of two statistics, 
term frequency and inverse document frequency.
The term frequency indicates how often a given term occurs in a document,
while the inverse document frequency quantifies the specificity of the term.
That is, a term is more specific if it occurs less frequently in the documents of the corpus.  
%
To calculate the tf-idf score of a document for a given query, 
we sum over the tf-idf scores of the individual words in the query.
We use the following formula to calculate these scores: 

\begin{equation*}
tfidf(t,d) = \log(1 + tf(t,d)) \cdot \log\frac{n}{df(t)}
\end{equation*}

We use $\log(1 + tf(t,d))$ instead of the raw term frequency $tf(t,d)$
to account for the fact that relevance	does not increases proportionally with the term frequency. The document frequency is calculated as $df(t) = \#\{d:tf(t,d) > 0\}$.


\subsubsection{BM25}

The BM25 model is slightly more advanced than the tf\textendash idf model;
in addition to term frequency and inverse document frequency
it also includes a component that controls for the document length.
%
To calculate the BM25 score of a document for a given query, 
we sum over the BM25 scores of the unique words in the query.
We use the following formula to calculate these scores: 

\begin{equation*}
BM25(t,d) = \frac{(k + 1) tf(t,d)}{k(1 - b + b \cdot \frac{l_d}{l_{avg}} ) + tf(t,d)} \cdot \log\frac{n}{df(t)} 
\end{equation*}

The values $l_d$ and $l_{avg}$ represent the document length and the average
document length, respectively.
We use $k = 1.5$ and $b = 0.75$ in our evaluation as commonly used default values.
 
\subsubsection{Language Models}

A statistical language model is a probability distribution over word sequences.
To score a given document-query pair, we calculate the query likelihood
$p(q|d)$, which is proportional to $p(d|q)$, 
assuming a uniform prior for $p(d)$.
We use the unigram approximation to calculate the query probability.
That is, we calculate the probability of the query by
multiplying the probabilities of the individual words of the query
(in our actual implementation we sum over the log values to avoid underflow).

We consider three different language models that are different in the way that
they handle smoothing, i.e. adjusting the maximum likelihood estimate to avoid 
zero probability for unseen words.

\paragraph{Jelinek-Mercer}

The Jelinek-Mercer model calculates the probability of a word by
interpolating the language model based on the document with
a background language model based on the full corpus.
The formula is given below.

\begin{equation*}
p_{\lambda}(w | d) = \lambda \frac{tf(w,d)}{|d|} + (1 - \lambda) \frac{tf(w,C)}{|C|}
\end{equation*}


\paragraph{Dirichlet Prior}

Dirichlet prior smoothing also interpolates the maximum 
likelihood estimate with a background model,
but in this case the factor $\lambda$ depends on the document length;
the longer the document the lower the weight of the background component $(1-\lambda)$.
The formula for the Dirichlet model is given below:

\begin{equation*}
p_{\mu}(w | d) = \frac{tf(w,d)}{|d| + \mu}  + \frac{\mu}{\mu + |d|} \frac{tf(w,C)}{|C|}  
\end{equation*}


\paragraph{Absolute Discounting}

Absolute discounting lowers the probability of seen words by substracting a constant value from their counts.
The formula is given below, with $|d|$ the length of the document and $|d|_u$ the number of unique words:

\begin{equation*}
p_{\delta}(w | d) = \frac{\max(tf(w,d) - \delta, 0)}{|d|} + \frac{\delta |d|_{u}}{|d|} \frac{tf(w,C)}{|C|}  
\end{equation*}


\subsubsection{Positional Language Model}

The positional language model differs from the other
language models by the fact that it takes the proximity
of query term occurrences into account. That is,
it rewards documents with matching query terms
close to each other.
%
A language model for each position in a given document 
is calculated from the words in the document discounted
based on their kernel distance to the position.
The final score for a word is determined by the 
propagated score of the word at the best position.
A detailed explanation of the positional language model can 
be found in \cite{PLM}.

We implemented the positional language model
with the best position strategy and dirichlet prior smoothing.
We considered five different kernel functions:
Gaussian, Triangle, Cosine, Circle and Passage.
The formulas for these functions are in \cite{PLM},
except for the `Passage' kernel which
is given by $I(|i - j| <= \sigma)$.
We implemented these kernels with $\sigma = 50$ fixed.
We determine the final score of a document query pair
as the score of the multiplied probabilities of the words 
at the best position (for computational reasons we actually use 
the log of the sum). 


\paragraph{Implementation}

Given a kernel function $k$ and the indicator function 
$c(w,j)$ of the occurence of a word $w$ at position $i$,
the formula for the positional language model is:

\begin{equation*}
p(w|D,i) = \frac{\sum_{j=1}^N c(w,j)k(i,j)}
{\sum_{w' \in V} \sum_{j=1}^N c(w',j)k(i,j)} 
\end{equation*}

Calculating the $p(q|D,i)$ for each position
in order to determine the $p(q|D,i)$ at the 
best position is computationally expensive.
For this reason, we optimized the implementation.

First, we only consider positions in the (inclusive) interval
between the left most occurrence and the right most
occurrence of a query term in the document.
We can savely assume that the best position is within this interval
since the excluded positions have lower probability
than the left and right most positions,
by nature of the kernel functions.

Secondly, we apply the optimization 
described in \cite{PLM} for calculating
$\sum_{w' \in V} \sum_{j=1}^N c(w',j)k(i,j)$.
That is, we actually calculate 
$\sum_{j=1}^N k(j,i)$ which is shown to be equivalent.

Finally, we cache the values that we calculate for 
$\sum_{j=1}^N k(j,i)$. That is,
we build a cache that returns the
value for $\sum_{j=1}^N k(j,i)$ given
a kernel function $k$, a document length $N$
and a position $i$. The cache
significantly reduced the run time
for our test and evaluation runs.
For example, running the positional
language model on the test set 
consisting of 49.195 query document pairs
with on average 4.2 query terms per query,
took approximately five minutes
on a laptop with an intel i7 processor.

\subsection{Offline Evaluation}
\label{LexicalModels:Evaluation}

We compare the retrieval models that we implemented
using offline evaluation. Below we describe
our experimental setup (Section \ref{ExperimentalSetup}), 
the results that we obtained for our lexical retrieval models 
(Section \ref{ResultsLexicalModels}), 
and we provide an analysis of these results 
(Section \ref{AnalysisLexicalModels}).

\subsubsection{Experimental Setup}
\label{ExperimentalSetup}

%%%%%%%%%%%% Test and Validation Data

\paragraph{Test and Validation Set}

We create a testset of query-document pairs from a file
containing queries with labeled documents. The labels tell whether
a document is considered relevant or not for the given query.
The testset consists of 119 queries with an average of 
413 labeled documents per query. 
The average query length is 4.2 words.
%
For optimizing the hyper parameters of our retrieval models
we build a validation set from a second file with different 
labeled query-document pairs. 
The validation set consists of 29 queries with an average of 
464 labeled documents per query. 
The average query length for the validation set is 4.1 words.

\paragraph{Comparing Retrieval Methods}

%%%%%%%%%%%% Evaluation pipeline
We run the retrieval models on the query-document pairs from the
testset to score the relevance of the document for the query.
By ordering the documents based on this `relevance' score
we construct a ranking of documents per query.
We then compare our ranking with the relevance labels using
the TREC Eval utility\footnote{\url{https://github.com/usnistgov/trec_eval}}. TREC Eval is the de facto standard way to compute Information Retrieval measures. TREC Eval computes evaluation scores
for a ranking of documents per query, given
ground-truth information regarding the relevance of the documents
for the given query. 

\paragraph{Metrics}

%%%%%%%%%% Metrics: ndcg
TREC Eval produces many different metrics that can be used 
to estimate the quality of a ranking result.
We choose normalized discounted cumulative gain (nDCG) as
our primary metric of interest for comparison of results.
(n)DCG accumulates the gain of documents from the top of the ranking
to the bottom, with the gain of each result discounted at lower ranks.
We compare nDCG scores at rank 10 (nDCG@10) since we consider the top ten results as most important assuming that they 'fit on a page' for a search engine. nDCG is often used to measure effectiveness of web search engine algorithms.

%%%%%%%%%% Metrics: others
In addition to nDCG@10 we also show results for some other metrics,
namely: Mean Average Precision at rank 1000 (MAP@1000), 
recall at rank 1000 (Recall@1000) and Precision at rank 5 (Precision@5).
We show these metrics to give a more complete picture and because they
may be relevant for other search scenarios.


\subsubsection{Results}
\label{ResultsLexicalModels}

We first optimize the hyper parameters of the 
language models and then generate scores for
all retrieval models discussed in 
Section \ref{LexicalModels:Models}.

\paragraph{Hyper Parameters}

We optimize the hyper parameters of the language models
by comparing the average nDCG@10 scores on the validation set 
using different values for the hyper parameters.
The results are shown in Figure \ref{Fig:OptimizingHyperParams}.
Unfortunately, the results do not look generalizable and 
from these figures it is not clear what 
the best parameter values should be. 

\input{content/fig_hyperparams}

We decided on the actual value that we use in the rest of the experiment
by comparing three values and picking the value that gave the best result.
%
For Jelinek-Mercer we compare the scores for 0.1 (ndcg 0.449),
0.5 (ndcg 0.465) and 0.9 (ndcg 0.464). Based on these results we pick
the parameter value 0.5. 
%
For Absolute Discounting we compare the scores for 0.1 (ndcg 0.450),
0.5 (ndcg 0.457) and 0.9 (ndcg 0.473). Based on these results we pick
the parameter value 0.9. 
%
For Dirichlet prior we compare the scores for 500 (ndcg 0.482),
1000 (ndcg 0.486) and 1500 (ndcg 0.483). Based on these results we pick
the parameter value 1000. 




For the positional language model we compare combinations of
kernel functions and hyperparameter values for dirichlet smoothing.
We leave the hyper parameter for the kernel functions ($\sigma$) fixed at 50.
The results are summarized in Table \ref{tbl_plm}.
The passage kernel with dirichlet parameter 500 has the highest
average nDCG@10 score, namely 0.476. We did not look at the
significance of this result since we are interested
in finding a reasonable configuration for the positional language model, not in proofing that one kernel/parameter combination is better than another.
Thus, based on the results shown in Table \ref{tbl_plm}  we use for the rest of the experiment the positional language model with a passage kernel and dirichlet smoothing parameter value 500.

\input{content/tbl_plm_hyperparams}

\paragraph{Results}

We compare the performance of the retrieval models by applying them on the query-document pairs in the test set. 
As a result of running our evaluation we generate scores per query for each retrieval function
for different metrics. The mean of these values are shown in Table \ref{tbl_means}. 
We have decided on nDCG@10 as our main metric of interest beforehand, when we set up the experiment. If we compare the results for nDCG@10 we see that TF-IDF has the highest
value (0.490), followed by Dirichlet (0.486), BM25 (0.479), Absolute Discounting (0.473),
and Jelinek-Mercer (0.465) as the least performing retrieval model.
In the next paragraph we discuss the significance 
of these results

\input{content/tbl_means}

\paragraph{Significance}

We use a two-tailed paired t-test to decide whether the observed differences in the means of the nDCG@10 scores (Table \ref{tbl_means}, row nDCG@10) are significant.
We use the paired t-test because the samples of scores are related,
that is, they are for the same queries.
We use a two-tailed test because we are interested in determining
whether there exists a significant difference in any direction.
We use a significance level of $\alpha = 0.05$ as this significance level is widely used in practice.

\input{content/tbl_significance}

The resulting p-values of the two-tailed paired t-test are shown in \ref{tbl_pvalues}, the highlighted cells show the p-values that are below the significance level. Thus, the only significant difference that we found for the retrieval methods that we studied is between the methods Dirichlet-1000 and Jelinek-Mercer-0.5.
Notice that TF-IDF has a higher average mean than Dirichlet-1000 but 
does not significantly performs better than Jelinek-Mercer (p-value: 0.155).
This is possible since the distribution for the differences between TF-IDF 
and Jelinek-Mercer differs from the distribution of differences between
Dirichlet-1000 and Jelinek-Mercer, that is, these distributions have a different 
standard deviation.

Dirichlet-1000 has a higher average nDCG@10 value than Jelinek-Mercer 
(0.486 versus 0.465; Table \ref{tbl_means}), and this result is significant
given a significance level of 0.05 (p-value 0.013, Table \ref{tbl_pvalues}).
Thus, can we now conclude that Dirichlet-1000 performs significantly better than Jelinek-Mercer? To answer this question we have to discuss the multiple comparison problem.
The multiple comparison problem states that the chance of obtaining at least one ''statistically significant'' result is greater than the significance level set for each individual comparison (even if all null hypotheses are true).
In order to avoid a lot of false positives, the significance level needs to be lowered to account for the number of comparisons being performed.

A conservative approach is the Bonferroni 
correction\footnote{\url{http://mathworld.wolfram.com/BonferroniCorrection.html}}, 
which sets the significance level for the entire set of $n$ comparisons equal to $\alpha$ by setting the $\alpha$ values for each comparison equal to $\frac{alpha}{n}$.
In our case, we compare six retrieval methods pairwise, wich results in
$n = \frac{6!}{2!(6-2)!} = 15$ comparisons. Thus, when we apply the Bonferri
correction we obtain a corrected significance level of
 $\alpha = \frac{0.05}{15} = 0.0033$. According to this new significance level
 we can not reject any null hypothesis, that is, we can not reject the
 hypothesis that all our retrival methods have the same performance.


\subsubsection{Analysis}
\label{AnalysisLexicalModels}







- manual inspection



- Do all methods perform similarly on all queries? Why?

- Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?

- note on performance